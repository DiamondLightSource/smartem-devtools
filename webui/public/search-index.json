{
  "version": 1,
  "generatedAt": "2026-01-20T18:02:14.062Z",
  "documentCount": 51,
  "documents": [
    {
      "id": "doc-1",
      "title": "SmartEM Agent CLI Reference",
      "href": "/docs/agent/cli-reference",
      "section": "Agent",
      "content": "SmartEM Agent CLI Reference\n\nThe SmartEM Agent provides a comprehensive command-line interface for parsing, validating, and monitoring EPU (Electron Physical User) microscopy data. This reference documents all available commands, parameters, and usage patterns.\n\nCommand Overview\n\nThe CLI is organised into the following command groups:\n\nparse - Parse various EPU data files and structures\nvalidate - Validate EPU project directory structure\nwatch - Monitor directories for real-time data processing\n\nGlobal Options\n\nThe following verbosity options are supported by the  command:\n\n Short  Default \n\n   0 \n\nVerbosity Levels:\n0 (default): ERROR level - Only critical errors are shown\n1 (-v): INFO level - General information and warnings\n2 (-vv): DEBUG level - Detailed debugging information\n\nNote: The / flag currently only works with the  command. Parse and validate commands use default logging levels. This is a known limitation.\n\nParse Commands\n\nParse commands extract and process data from EPU files without persisting to the backend API. These are primarily used for development, debugging, and data validation purposes.\n\nparse dir\n\nParse an entire EPU output directory structure that may contain multiple grids.\n\nParameters:\n\n Type  Description ---------------------\n str  Path to EPU output directory containing multiple grid directories  Parameter  Required \n-------------------   Yes \n\nExample:\n\nUse Cases:\nSingle grid analysis and debugging\nGrid-specific data extraction\nVerification of individual acquisition datasets\n\nparse session\n\nParse an EPU session manifest file (typically ).\n\nParameters:\n\n Type  Description ---------------------\n str  Path to EPU session manifest file  Parameter  Required \n-------------------   Yes \n\nExample:\n\nOutput: Atlas information including grid square positions, tile arrangements, and overview image metadata.\n\nparse gridsquare-metadata\n\nParse grid square metadata files (typically ).\n\nParameters:\n\n Type  Description ---------------------\n str  Path to grid square metadata file  Parameter  Required \n-------------------   Yes \n\nExample:\n\nOutput: Grid square acquisition details and image collection parameters.\n\nparse foilhole\n\nParse foil hole manifest files (typically ).\n\nParameters:\n\n Type  Description ---------------------\n str  Path to foil hole manifest file  Parameter  Required \n-------------------   Yes \n\nExample:\n\nOutput: Individual micrograph metadata including exposure parameters and image quality metrics.\n\nValidate Command\n\nThe validate command checks EPU project directory structure for completeness and compliance with expected formats.\n\nParameters:\n\n Type  Default \n\n str  - \n\nExit Codes:\n0: Directory structure is valid\n1: Directory structure is invalid (validation errors found)\n\nExample:\n\nValidation Checks:\nPresence of required EPU session files\nDirectory structure compliance\nFile naming convention adherence\nMetadata file accessibility\n\nOutput:\nValid structure: Confirmation message with no errors\nInvalid structure: Detailed list of structural issues and missing components\n\nWatch Command\n\nThe watch command provides real-time monitoring of EPU data directories, automatically processing new files and communicating with the SmartEM backend.\n\nParameters\n\n Type  Default \n\n Path  - \n bool  False \n str   \n str   \n float  10.0 \n str  None \n str  None \n int  30 \n int  60 \n count  0 \n\nCore Parameters\n\npath\nType: Path (required)\nDescription: The root directory to monitor for EPU data files. Must be an existing directory containing or expected to contain EPU acquisition data.\n\nExample:\n\n--dry-run\nType: Boolean flag\nDefault: False\nDescription: Run the agent in simulation mode without making API calls to the backend. Useful for testing file monitoring and parsing without affecting production systems.\n\nExample:\n\nBehaviour:\nFile monitoring and parsing operate normally\nNo data is sent to the backend API\nUses in-memory data storage only\nIdeal for development and testing scenarios\n\nAPI Integration Parameters\n\n--api-url\nType: String\nDefault: \nDescription: URL of the SmartEM backend API endpoint. The agent will attempt to connect to this URL for data persistence and real-time communication.\n\nExamples:\n\nConnection Validation: The agent validates API connectivity at startup and exits with error code 1 if the API is unreachable (unless using ).\n\n--agent-id\nType: String (optional)\nDescription: Unique identifier for this agent instance. Required for real-time communication with the backend via Server-Sent Events (SSE).\n\nFormat Recommendations:\nUse descriptive names: , \nInclude facility/location information for multi-site deployments\nEnsure uniqueness across all agent instances\n\n--session-id\nType: String (optional)\nDescription: Session identifier linking this agent to a specific microscopy session. Required for real-time communication with the backend.\n\nUsage: Typically generated by the backend or provided by the acquisition software. Used to group related data and instructions.\n\nReal-Time Communication Parameter",
      "excerpt": "SmartEM Agent CLI Reference The SmartEM Agent provides a comprehensive command-line interface for parsing, validating, and monitoring EPU (Electron Physical User) microscopy data. This reference docum..."
    },
    {
      "id": "doc-2",
      "title": "Run SmartEM Agent (EPU Agent)",
      "href": "/docs/agent/deployment",
      "section": "Agent",
      "content": "Run SmartEM Agent (EPU Agent)\n\nThe SmartEM Agent is a data collection service that monitors EPU (Electron Physical User) output directories and communicates acquisition data to the backend service in real-time.\n\nOverview\n\nThe EPU agent runs on EPU workstations either as a Python script or bundled Windows binary. EPU workstations are typically Windows machines isolated from the main network, where specific connectivity is achieved through a proxy and configured via an allow-list. The primary purpose of the EPU agent is to parse EPU software output from the filesystem and communicate data and events to the core backend component.\n\nAn EPU data directory is generated by closed-source EPU software and represents an acquisition session using a cryo-electron microscope. The SmartEM Agent provides comprehensive capabilities for processing this data:\n\nCore Capabilities\n\nReal-time monitoring: Watch EPU directories for file changes during active acquisitions\nComprehensive parsing: Extract data from all EPU file types (sessions, atlases, grid squares, foil holes, micrographs)\nData validation: Verify EPU directory structure and completeness\nBackend integration: Communicate with SmartEM backend via REST API and Server-Sent Events (SSE)\nConnection health: Automatic heartbeat monitoring for reliable data transmission\nFlexible deployment: Run in development, testing, or production modes\n\nAgent Modes\n\nThe agent operates in several modes depending on the timing of EPU data acquisition:\n\nPre-acquisition mode: Watcher launched before EPU starts writing - real-time monitoring only\nMid-acquisition mode: Watcher launched after EPU starts writing - combines parsing existing files with real-time monitoring\nPost-acquisition mode: Watcher launched after EPU finishes - parses complete dataset then monitors for changes\n\nQuick Start\n\nFor comprehensive parameter documentation, see the CLI Reference. For troubleshooting, see the CLI Troubleshooting Guide.\n\nBasic Directory Monitoring\n\nProduction Deployment with Backend Integration\n\nCommand Categories\n\nThe SmartEM Agent CLI is organised into three main command categories:\n\nParse Commands\nExtract and analyse data from EPU files without backend communication. Useful for development, debugging, and data validation.\n\nValidate Commands\nCheck EPU directory structure for completeness and compliance with expected formats.\n\nWatch Commands\nMonitor directories in real-time for file changes with full backend integration.\n\nParsing Operations\n\nParse commands extract and analyse data from EPU files without communicating with the backend API. These commands are ideal for development, debugging, data validation, and understanding EPU data structures.\n\nComplete Directory Parsing\n\nParse entire EPU directories containing multiple grids or complete acquisition sessions:\n\nIndividual Component Parsing\n\nParse specific EPU file types to understand data structures and debug issues:\n\nSession Files\n\nAtlas Files\n\nGrid Square Files\n\nFoil Hole Files\n\nValidation Operations\n\nValidation commands check EPU directory structure for completeness and compliance with expected formats.\n\nExamples with Expected Outcomes\n\nInvalid Directories (Expected to Fail)\nThese examples demonstrate directories with structural issues:\n\nValid Directories (Expected to Pass)\nThese examples show properly structured EPU directories:\n\nUnderstanding Validation Results\n\nSuccessful validation returns exit code 0 and confirms the directory structure is valid:\n\nFailed validation returns exit code 1 and lists specific issues:\n\nReal-Time Monitoring (Watch Operations)\n\nThe watch command provides real-time monitoring of EPU directories, automatically processing new files and communicating with the SmartEM backend. This is the primary operational mode for production deployments.\n\nBasic Watch Operations\n\nProduction Monitoring with Backend Integration\n\nWatch Operation Modes\n\nThe watch command is designed to gracefully handle different timing scenarios relative to EPU data acquisition:\n\nPre-acquisition mode: Watcher launched before EPU starts writing\n   - Only real-time monitoring is necessary\n   - Files are processed as they are created\n   - Most efficient mode for active acquisitions\n\nMid-acquisition mode: Watcher launched after EPU starts writing\n   - Combines initial parsing of existing files with real-time monitoring\n   - Automatically detects and processes pre-existing data\n   - Seamlessly transitions to real-time monitoring\n\nPost-acquisition mode: Watcher launched after EPU finishes\n   - Parses complete dataset then monitors for any changes\n   - Useful for processing archived or completed datasets\n   - Continues monitoring for potential updates\n\nTesting with Simulated EPU Data\n\nFor development and testing, use the epuplayer tool to simulate realistic EPU file writing patterns:\n\nRecording EPU Patterns\n\nReplaying EPU Patterns\n\nQuick Testing Alternative\n\nImportant Considerations\n\nCritical File: The  file is essential for proper operation as it:\nProvides referenc",
      "excerpt": "Run SmartEM Agent (EPU Agent) The SmartEM Agent is a data collection service that monitors EPU (Electron Physical User) output directories and communicates acquisition data to the backend service in r..."
    },
    {
      "id": "doc-3",
      "title": "Agent",
      "href": "/docs/agent",
      "section": "Agent",
      "content": "Agent\n\nDocumentation for the SmartEM Agent - the EPU filesystem watcher that runs on Windows workstations near the microscope.\n\nTopics\n\nUsage\n\nCLI Reference - Command-line interface documentation and options\nTroubleshooting - Common issues and solutions\n\nDeployment\n\nAgent Deployment - Running the agent on Windows workstations",
      "excerpt": "Agent Documentation for the SmartEM Agent - the EPU filesystem watcher that runs on Windows workstations near the microscope. Topics Usage CLI Reference - Command-line interface documentation and opti..."
    },
    {
      "id": "doc-4",
      "title": "SmartEM CLI Troubleshooting Guide",
      "href": "/docs/agent/troubleshooting",
      "section": "Agent",
      "content": "SmartEM CLI Troubleshooting Guide\n\nThis guide provides solutions for common issues encountered when using the SmartEM Agent command-line interface. For comprehensive parameter documentation, see the CLI Reference.\n\nQuick Diagnostics\n\nCheck CLI Installation\n\nTest Basic Functionality\n\nCommon Issues and Solutions\n\nCommand Not Found Errors\n\nError: \n\nCause: SmartEM Agent package is not installed or not in Python path.\n\nSolutions:\n\nAlternative: Use the full path to the module:\n\nError: \n\nCause: Trying to run as a script instead of a module.\n\nSolution: Use the module syntax:\n\nDirectory and File Access Issues\n\nError: \n\nDiagnosis:\n\nSolutions:\nUse absolute paths: \nVerify directory spelling and case sensitivity\nCheck directory permissions: \n\nError: \n\nCause: Insufficient permissions to read directories or write log files.\n\nSolutions:\n\nError: Files not being detected during watch\n\nDiagnosis:\n\nSolutions:\nVerify file patterns match EPU naming conventions\nCheck for symbolic links that might not be followed\nEnsure files are completely written before processing\nConsider filesystem-specific issues (NFS, network drives)\n\nAPI Connection Problems\n\nError: \n\nDiagnosis:\n\nSolutions:\n\nStart the backend API:\n\nUse correct API URL:\n\nNetwork connectivity issues:\n   - Check firewall settings\n   - Verify proxy configuration\n   - Test with different network interfaces\n   - Use  for offline testing\n\nError:  or \n\nDiagnosis:\n\nSolutions:\n\nVerify agent and session IDs:\n\nAdjust timeout settings:\n\nCheck backend logs:\n   - Monitor backend API logs for connection errors\n   - Verify database connectivity\n   - Check for authentication issues\n\nParsing and Validation Errors\n\nError: \n\nDiagnosis:\n\nCommon Issues:\nMissing  file\nIncorrect directory naming conventions\nIncomplete or corrupted files\nWrong directory structure (not EPU format)\n\nSolutions:\nVerify the directory contains valid EPU data\nCheck for required files: , \nEnsure directory structure matches EPU conventions\nUse  command to identify specific parsing issues\n\nError:  or parsing failures\n\nDiagnosis:\n\nSolutions:\nVerify files are not corrupted or partially written\nCheck file permissions and accessibility\nEnsure files are in expected format (not binary corrupted)\nLook for encoding issues or special characters in paths\n\nPerformance and Resource Issues\n\nIssue: High memory usage or slow processing\n\nDiagnosis:\n\nSolutions:\n\nOptimise logging settings:\n\nProcess smaller datasets:\n\nSystem resource limits:\n\nIssue: \n\nSolutions:\n\nLogging and Output Issues\n\nIssue: No log output or missing log files\n\nDiagnosis:\n\nSolutions:\nEnsure log directory exists and is writable\nUse absolute paths for log files\nCheck disk space availability\nVerify SELinux/AppArmor policies if applicable\n\nIssue: Verbose output not showing\n\nSolutions:\n\nSignal Handling and Process Management\n\nIssue: Process doesn't stop gracefully with Ctrl+C\n\nSolutions:\n\nIssue: Background process monitoring\n\nSolutions:\n\nAdvanced Troubleshooting\n\nDebug Mode Activation\n\nEnable maximum debugging output:\n\nNetwork Debugging\n\nTest connectivity chain:\n\nFile System Monitoring Debug\n\nManual file monitoring:\n\nDatabase Connectivity Issues\n\nTest database connection (if using persistent storage):\n\nEnvironment-Specific Issues\n\nWindows Specific\n\nPath separator issues:\n\nService account permissions:\nEnsure service account has appropriate file system access\nCheck Windows Defender exclusions for monitoring directories\nVerify no Group Policy restrictions on file access\n\nLinux/Unix Specific\n\nPermission issues:\n\nNFS mounted directories:\n\nContainer/Docker Environments\n\nVolume mounting issues:\n\nNetwork connectivity in containers:\n\nGetting Additional Help\n\nCollecting Diagnostic Information\n\nCreate a diagnostic script:\n\nLog Analysis\n\nExtract relevant log entries:\n\nReporting Issues\n\nWhen reporting issues, include:\n\nCommand used: Full command line with parameters\nError message: Complete error output\nEnvironment: OS, Python version, installation method\nDirectory structure: Sample of the directory being processed\nLogs: Relevant log entries with timestamps\nNetwork information: API URLs, connectivity status\nSystem resources: Available memory, disk space\n\nExample issue report:\n\nThis comprehensive troubleshooting guide should resolve most common CLI issues. For backend-specific problems, consult the SmartEM Backend documentation.",
      "excerpt": "SmartEM CLI Troubleshooting Guide This guide provides solutions for common issues encountered when using the SmartEM Agent command-line interface. For comprehensive parameter documentation, see the CL..."
    },
    {
      "id": "doc-5",
      "title": "Architecture",
      "href": "/docs/architecture",
      "section": "Architecture",
      "content": "Architecture\n\nSystem design documentation and architectural decision records.\n\nDecision Records\n\nArchitectural Decision Records (ADRs) document significant technical decisions.\n\nDecision Records - Browse all ADRs\n\nSystem Design\n\nArchitecture documentation is maintained in the main smartem-decisions repository:\n\nSystem overview and component interactions\nBackend-Agent communication protocols (SSE, REST)\nData flow and message queue patterns\nDatabase schema design",
      "excerpt": "Architecture System design documentation and architectural decision records. Decision Records Architectural Decision Records (ADRs) document significant technical decisions. Decision Records - Browse..."
    },
    {
      "id": "doc-6",
      "title": "Athena Integration",
      "href": "/docs/athena",
      "section": "Athena",
      "content": "Athena Integration\n\nDocumentation for ThermoFisher Athena API integration.\n\nAPI Reference\n\nThe Athena Decision Service API specification is available in the API documentation:\n\nAthena API Docs - Interactive Swagger UI\nAPI Documentation Guide - How to use the interactive documentation\n\nMock Server\n\nFor local development without access to a real Athena service:\n\nSee the API Documentation guide for more details.",
      "excerpt": "Athena Integration Documentation for ThermoFisher Athena API integration. API Reference The Athena Decision Service API specification is available in the API documentation: Athena API Docs - Interacti..."
    },
    {
      "id": "doc-7",
      "title": "Using the API Documentation",
      "href": "/docs/backend/api-documentation",
      "section": "Backend",
      "content": "Using the API Documentation\n\nThis guide explains how to use the interactive API documentation for SmartEM Decisions services.\n\nOverview\n\nSmartEM Decisions provides interactive API documentation built with Swagger UI, giving you:\n\nLive API exploration - Try endpoints directly in your browser\nComplete specifications - Full OpenAPI 3.0 documentation\nExample requests/responses - See exactly what data to send and expect\nAuthentication helpers - Built-in tools for API authentication\n\nAccessing the Documentation\n\nOnline Documentation\n\nVisit the hosted API documentation at:\nAthena Decision Service: Athena API Docs\nSmartEM Core API: SmartEM API Docs\n\nLocal Documentation\n\nTo run the documentation locally with a live mock server:\n\nUsing the Interactive Features\n\nTry It Out\n\nExpand an endpoint by clicking on it\nClick \"Try it out\" to enable the interactive form\nFill in parameters using the provided form fields\nClick \"Execute\" to make a real API call\nView the response including status code, headers, and body\n\nAuthentication\n\nFor endpoints requiring authentication:\n\nClick the \"Authorize\" button at the top of the page\nEnter your credentials (API key, bearer token, etc.)\nClick \"Authorize\" to save credentials for all requests\n\nRequest Examples\n\nEach endpoint shows:\nParameter descriptions - What each field does\nExample values - Sample data to help you understand the format\nSchema definitions - Complete data structure specifications\nResponse examples - What to expect back from the API\n\nAPI Specifications\n\nUnderstanding API Sources\n\nAthena Decision Service API - External service integration:\nSource of Truth: External Athena service OpenAPI specification ()\nOur Implementation: Python client and mock server generated from spec\nDocumentation: Based on the authoritative external specification\n\nSmartEM Core API - Our implementation:\nSource of Truth: Our FastAPI/Django implementation\nOur Implementation: Backend service with business logic\nDocumentation: Generated from our actual API endpoints\n\nDownload Specifications\n\nYou can download the raw OpenAPI specifications:\n\nAthena API Spec - Official external specification\nAthena Source Spec - Original specification file\nSmartEM API Spec - Generated from our implementation\n\nUsing Specifications\n\nImport these into your favorite API tools:\nPostman - Import → Link → Paste URL\nInsomnia - Create → Import/Export → From URL\nVS Code REST Client - Use with OpenAPI extensions\nCode generators - Generate client libraries in various languages\n\nCommon Workflows\n\nTesting Decision Service Integration\n\nStart with session management:\n\nRegister areas:\n\nRecord decisions:\n\nStore algorithm results:\n\nMonitoring Acquisition Status\n\nCheck application state:\n\nQuery area states:\n\nRetrieve decisions:\n\nDevelopment Tips\n\nMock Server for Development\n\nThe Athena API includes a full mock server for development:\n\nClient Library Usage\n\nUse the Python client library for programmatic access:\n\nError Handling\n\nThe API follows standard HTTP status codes:\n\n200-299: Success responses\n400-499: Client errors (bad requests, missing data)\n500-599: Server errors (system failures)\n\nCheck the API documentation for specific error responses and how to handle them.\n\nSupport\n\nFor API-related questions:\n\nCheck the interactive documentation for endpoint details\nReview the example code in this documentation\nExamine the client library source code for usage patterns\nOpen an issue on the GitHub repository\n\nThe API documentation is automatically updated with each release, ensuring you always have access to the latest features and changes.",
      "excerpt": "Using the API Documentation This guide explains how to use the interactive API documentation for SmartEM Decisions services. Overview SmartEM Decisions provides interactive API documentation built wit..."
    },
    {
      "id": "doc-8",
      "title": "Run SmartEM Backend",
      "href": "/docs/backend/api-server",
      "section": "Backend",
      "content": "Run SmartEM Backend\n\nThe core backend service providing HTTP API, database operations, and message queue processing for intelligent cryo-EM data collection.\n\nBackend Operations",
      "excerpt": "Run SmartEM Backend The core backend service providing HTTP API, database operations, and message queue processing for intelligent cryo-EM data collection. Backend Operations"
    },
    {
      "id": "doc-9",
      "title": "Database Migrations",
      "href": "/docs/backend/database",
      "section": "Backend",
      "content": "Database Migrations\n\nThis guide explains how to use database migrations in the smartem-decisions project.\n\nOverview\n\nWe use Alembic for database schema versioning and migrations. Alembic integrates with our SQLModel-based database models to provide:\n\nSchema versioning: Track database changes over time\nAutomated migration generation: Generate migrations from model changes\nRollback support: Safely revert database changes\nEnvironment-specific deployments: Apply migrations across development, staging, and production\n\nPrerequisites\n\nEnsure you have the backend dependencies installed:\n\nThis includes Alembic as a dependency.\n\nBasic Usage\n\nInitial Database Setup\n\nFor a completely new database, run the following command to apply all migrations from scratch:\n\nThis will:\nCreate the Alembic version tracking table\nApply migration 001: Create core SmartEM schema baseline (acquisition, grid, gridsquare, micrograph, foilhole, atlas, etc.)\nApply migration 002: Add performance indexes for acquisition datetime queries\nApply migration 003: Add prediction model tables\nApply remaining migrations for quality metrics, training tables, and schema fixes\n\nRunning Migrations\n\nApply all pending migrations to bring your database up to the latest schema:\n\nChecking Migration Status\n\nSee the current database version:\n\nView migration history:\n\nRolling Back Migrations\n\nDowngrade to a specific revision:\n\nRollback one migration:\n\nCreating New Migrations\n\nAuto-generating Migrations\n\nWhen you modify SQLModel classes in , generate a migration automatically:\n\nImportant: Always review auto-generated migrations before applying them. Alembic may not detect all changes (like column renames or complex constraints).\n\nManual Migrations\n\nFor data migrations or complex schema changes, create an empty migration:\n\nEdit the generated file in  to add your custom logic.\n\nMigration Examples\n\nStructure Changes\n\nIndex Changes\n\nData Changes\n\nBest Practices\n\nMigration Safety\n\nAlways backup production data before running migrations\nTest migrations on a copy of production data first\nReview auto-generated migrations carefully before applying\nWrite reversible migrations with proper downgrade logic\n\nSchema Changes\n\nAdd columns as nullable first, then make them non-nullable in a separate migration if needed\nUse separate migrations for structure and data changes\nInclude appropriate indexes for performance-critical fields\nDocument breaking changes in migration comments\n\nDevelopment Workflow\n\nCreate feature branch for database changes\nGenerate migration after modifying models\nTest migration locally (up and down)\nReview migration code before committing\nCoordinate with team for schema changes affecting multiple developers\n\nDeployment\n\nProduction Deployment\n\nStaging Environment\n\nTroubleshooting\n\nCommon Issues\n\nMigration conflicts: When multiple developers create migrations simultaneously:\n\nFailed migration: If a migration fails partway through:\n\nModel out of sync: When models don't match database:\n\nMigration-Specific Issues\n\nMissing baseline schema: If you encounter errors like \"relation 'gridsquare' does not exist\" when running migrations:\n\nThis indicates that migration 002 is trying to create indexes on tables that don't exist yet. This happens when the baseline schema migration is missing or not properly applied. The fix is to ensure the baseline migration (6e6302f1ccb6) is applied before migration 002.\n\nEnum type conflicts: If you encounter \"type already exists\" errors:\n\nThis can happen when enum types exist from previous schema creation attempts but don't match the migration expectations.\n\nDatabase Connection Issues\n\nThe migration system uses your existing database connection configuration from .\n\nEnsure your environment variables are set:\nDatabase connection parameters\nPostgreSQL credentials\nNetwork access to database server\n\nFiles and Structure\n\nMigration file naming convention:  where:\nis the creation timestamp\nis the sequential migration number\ndescribes the change\n\nFurther Reading\n\nAlembic Documentation\nSQLModel Documentation\nPostgreSQL Documentation",
      "excerpt": "Database Migrations This guide explains how to use database migrations in the smartem-decisions project. Overview We use Alembic for database schema versioning and migrations. Alembic integrates with..."
    },
    {
      "id": "doc-10",
      "title": "How to Use the SmartEM API Client",
      "href": "/docs/backend/http-api-client",
      "section": "Backend",
      "content": "How to Use the SmartEM API Client\n\nThis guide explains how to use the unified SmartEM API client to communicate with the SmartEM Core API.\n\nOverview\n\nThe SmartEM API Client provides a unified interface to interact with the SmartEM Core API. It supports both\nsynchronous and asynchronous operations, making it flexible for different usage scenarios. The client also includes\ndata conversion utilities to convert between EPU data models and API request/response models.\n\nInstallation\n\nThe SmartEM API Client is included with the SmartEM Decisions package, so no additional installation is required.\n\nBasic Usage\n\nImporting the Client\n\nCreating a Client Instance\n\nUsing the Client with Context Managers\n\nThe client supports context managers to ensure proper resource cleanup:\n\nSynchronous vs Asynchronous Methods\n\nThe client provides both synchronous and asynchronous methods for all operations. The asynchronous methods are\nprefixed with  (e.g.,  vs ).\n\nWorking with Acquisitions\n\nCreating an Acquisition\n\nRetrieving Acquisitions\n\nUpdating an Acquisition\n\nDeleting an Acquisition\n\nWorking with Hierarchical Data\n\nThe client supports the full hierarchy of entities:\n\nAcquisition\nGrid\nGrid Square\nFoil Hole\nMicrograph\n\nHere's an example of creating entities at each level:\n\nEntityStore Compatibility\n\nThe client maintains compatibility with the existing  API for seamless integration:\n\nError Handling\n\nThe client includes comprehensive error handling:\n\nLogging\n\nThe client includes detailed logging. You can configure the log level to control verbosity:\n\nAdvanced Usage\n\nWorking with Raw API Responses\n\nIf you need to work with the raw API responses rather than parsed models:\n\nManaging the ID Mapping Cache\n\nThe client maintains a cache of entity IDs to database IDs:\n\nClosing the Client\n\nAlways close the client when you're done to free up resources:",
      "excerpt": "How to Use the SmartEM API Client This guide explains how to use the unified SmartEM API client to communicate with the SmartEM Core API. Overview The SmartEM API Client provides a unified interface t..."
    },
    {
      "id": "doc-11",
      "title": "Backend",
      "href": "/docs/backend",
      "section": "Backend",
      "content": "Backend\n\nDocumentation for the SmartEM backend services: API server, database, and message queue.\n\nTopics\n\nRunning the Backend\n\nAPI Server - Running and configuring the FastAPI server locally\nDatabase - PostgreSQL setup, Alembic migrations, schema management\n\nAPI Usage\n\nAPI Documentation - Interactive Swagger UI documentation for SmartEM and Athena APIs\nHTTP API Client - Python client for programmatic API access",
      "excerpt": "Backend Documentation for the SmartEM backend services: API server, database, and message queue. Topics Running the Backend API Server - Running and configuring the FastAPI server locally Database - P..."
    },
    {
      "id": "doc-12",
      "title": "Database Schema Drift Prevention",
      "href": "/docs/database-schema-drift-prevention",
      "section": "Documentation",
      "content": "Database Schema Drift Prevention\n\nThis document explains the automated schema drift detection system implemented in SmartEM Decisions to ensure database schema consistency across environments.\n\nWhat is Schema Drift?\n\nSchema drift occurs when SQLModel definitions (your Python data models) change but the corresponding Alembic database migrations haven't been updated. This creates a mismatch between:\n\nWhat your code expects the database schema to look like (SQLModel definitions)\nWhat the actual database schema looks like (current migration state)\n\nThis can lead to runtime errors, deployment failures, and data integrity issues.\n\nHow Our Detection Works\n\nOur schema drift detection system automatically catches these mismatches in CI/CD by:\n\nSetting up a temporary database - Creates a clean PostgreSQL database for testing\nApplying existing migrations - Runs all current Alembic migrations to establish the \"official\" schema state\nComparing with SQLModel definitions - Uses Alembic's autogenerate feature to detect what new migrations would be needed\nReporting drift - Fails the CI build if any schema changes are detected\n\nWhen the Check Runs\n\nThe schema drift check runs automatically:\n\nOn every pull request to \nOn every push to \nOn manual workflow dispatch\nSkipped for draft PRs with  prefix\n\nWhat Happens When Drift is Detected\n\nIf schema drift is detected, the CI build will fail with a clear error message:\n\nHow to Fix Schema Drift\n\nWhen you modify SQLModel definitions (add/remove fields, change types, etc.), follow these steps:\n\nGenerate the Migration\n\nReview the Generated Migration\n\nAlembic will create a new file in . Always review this file to ensure:\n\nThe changes match your intentions\nNo unintended schema modifications are included\nData migration logic is correct (if needed)\n\nTest the Migration\n\nCommit the Migration\n\nRunning the Check Locally\n\nYou can run the schema drift check locally before pushing:\n\nThe script will:\nCreate a temporary database\nApply existing migrations\nCheck for drift\nClean up the temporary database\nExit with code 0 (success) or 1 (drift detected)\n\nAdvanced Usage\n\nCustom Database Configuration\n\nThe script uses environment variables from  or similar K8s environment files:\n\nDebugging Migration Issues\n\nIf you encounter issues with the drift detection:\n\nCheck migration files - Ensure all migration files are committed\nVerify database connectivity - Test connection with your PostgreSQL instance\nReview SQLModel imports - Make sure all models are properly imported in \nCheck Alembic configuration - Verify  and  are correctly configured\n\nCI/CD Integration Details\n\nThe schema drift check is implemented as a reusable GitHub Actions workflow:\n\nWorkflow file: \nCalled from: \nDependencies: Runs after basic checks pass\nServices: Uses PostgreSQL 15 service container\n\nBest Practices\n\nFor Developers\n\nAlways run locally first - Test schema changes before pushing\nDescriptive migration messages - Use clear, descriptive names for migrations\nSmall, focused changes - Avoid large schema changes in single migrations\nReview autogenerated migrations - Don't blindly trust autogenerate output\n\nFor Reviews\n\nWhen reviewing PRs with database changes:\n\nCheck for migration files - Ensure SQLModel changes include corresponding migrations\nReview migration logic - Verify the migration does what it claims\nConsider data safety - Check for potentially destructive operations\nTest migration reversal - Ensure migrations can be safely downgraded\n\nTroubleshooting\n\nCommon Issues\n\nIssue: \"Environment variable POSTGRES_X not set\"\nSolution: Ensure  is properly configured with database credentials\n\nIssue: \"Error creating temporary database\"\nSolution: Verify PostgreSQL is running and credentials are correct\n\nIssue: \"Error running existing migrations\"\nSolution: Check that all migration files are valid and database is accessible\n\nIssue: False positive drift detection\nSolution: Ensure all SQLModel classes are imported in the migration env.py\n\nGetting Help\n\nIf you encounter issues with the schema drift detection system:\n\nCheck the CI logs for specific error messages\nRun the check locally for faster debugging\nVerify your local database setup matches the expected configuration\nConsult the team for complex migration scenarios\n\nTechnical Implementation\n\nScript Architecture\n\nThe  script follows this flow:\n\nSecurity Considerations\n\nTemporary databases use unique names to avoid conflicts\nDatabase credentials are loaded from environment variables\nTemporary resources are always cleaned up, even on failure\nNo production data is accessed during drift detection\n\nThis system ensures that database schema changes are always properly tracked through migrations, maintaining consistency across development, testing, and production environments.",
      "excerpt": "Database Schema Drift Prevention This document explains the automated schema drift detection system implemented in SmartEM Decisions to ensure database schema consistency across environments. What is..."
    },
    {
      "id": "doc-13",
      "title": "Backend-to-Agent Communication System Design",
      "href": "/docs/decision-records/backend-agent-communication-system-design",
      "section": "Decision Records",
      "content": "Backend-to-Agent Communication System Design\n\nThis document provides comprehensive system design documentation for the backend-to-agent communication system\nimplementing ADR #8's hybrid SSE + HTTP architecture. This system enables real-time delivery of microscope control\ninstructions from Kubernetes-hosted backend services to Windows workstations controlling\ncryo-electron microscopes.\n\nArchitecture Overview\n\nSystem Context\n\nThe SmartEM Decisions platform operates in a distributed environment where backend services run in Kubernetes clusters\nwhilst agent services execute on Windows workstations directly connected to scientific equipment.\nThe communication system bridges this divide whilst meeting high-throughput requirements.\n\nImplementation Status: COMPLETED - This POC implementation provides a production-ready backend-to-agent\ncommunication system with full SSE streaming, RabbitMQ integration, database persistence, and comprehensive\nconnection management.\n\nService Architecture\n\nThe communication system employs a separate service approach rather than integrating directly with the main API\nservice. This design provides:\n\nIsolation of concerns: Communication logic remains separate from core business logic\nScalability independence: Communication service can scale independently based on connection load\nOperational simplicity: Monitoring and debugging of persistent connections without affecting main API\nResource management: Dedicated resources for managing long-lived SSE connections\n\nTechnical Stack Integration\n\nFastAPI Integration\n\nThe communication service leverages FastAPI's native SSE support through  and event streaming\npatterns:\n\nRabbitMQ Message Flow\n\nThe system integrates with the existing RabbitMQ infrastructure as an event communication backbone between ML components and the communication service:\n\nPostgreSQL Schema Design\n\nThe communication system extends the existing database schema with instruction tracking tables:\n\nComponent Interactions and Data Flows\n\nPrimary Communication Flow (SSE)\n\nThe primary communication path uses Server-Sent Events for efficient real-time instruction delivery:\n\nError Handling and Recovery\n\nThe system implements comprehensive error handling across multiple failure scenarios:\n\nScalability Design\n\nConnection Management\n\nThe system is designed to support one session per agent machine with a capacity of 20 concurrent SSE\nconnections. This design aligns with the facility's requirements of up to 20 microscope workstations, where each workstation controls\na single microscope.\n\nTheoretical Scaling Limits\n\nCurrent Architecture Bottlenecks:\n\nDatabase Write Performance: High-frequency instruction persistence and state updates may impact database performance\nDatabase Connection Pool: Connection pool limits for concurrent instruction storage and retrieval operations\nMemory Usage: Each SSE connection maintains in-memory state (~1-2MB per connection)\nRabbitMQ Throughput: Event notification capacity for real-time updates\n\nScaling Strategies:\n\nHorizontal Scaling: Deploy multiple communication service instances behind load balancer\nConnection Sharding: Distribute agents across service instances by agent ID hash\nResource Optimization: Implement connection pooling and memory-efficient streaming\nDatabase Optimization: Use connection pooling and read replicas for instruction queries\n\nPerformance Characteristics\n\nExpected Throughput:\nInstruction Frequency: 1 instruction per 30-120 seconds per agent during active data collection\nPeak Load: 20 agents × 2 instructions/minute = 40 instructions/minute system-wide (at maximum frequency)\nMessage Size: 1-10KB JSON payloads for microscope control instructions\nLatency Requirements: Sub-second delivery for real-time workflow efficiency\n\nImplementation Specifications\n\nSSE Streaming Service Design\n\nThe SSE streaming service implements persistent connections with automatic reconnection handling:\n\nHTTP Acknowledgement Endpoints\n\nHTTP acknowledgement endpoints provide reliable delivery confirmation:\n\nSSE Retry Implementation\n\nThe system implements robust SSE reconnection with exponential backoff:\n\nImplementation Status & Components\n\nThis POC implementation provides a complete working system with the following implemented components:\n\nCompleted Features\n\nDatabase Schema & Migration (Alembic)\nAgentSession: Session management for agent connections\nAgentInstruction: Instruction storage with metadata and lifecycle tracking\nAgentConnection: Real-time connection tracking with heartbeat monitoring\nAgentInstructionAcknowledgement: Comprehensive acknowledgement tracking\n\nFastAPI SSE Endpoints\n*: Real-time SSE streaming\n: HTTP acknowledgement\nDebug endpoints: Connection statistics and session management\n\nRabbitMQ Integration\nEvent Publishers: Agent instruction lifecycle events\nConsumer Handlers: Process instruction events and database updates\nMessage Types: , , \n\nEnhanced Agent Client ()\nExponential backoff retry logic with jitter\nConnection statistics and monitoring\nCompr",
      "excerpt": "Backend-to-Agent Communication System Design This document provides comprehensive system design documentation for the backend-to-agent communication system implementing ADR #8's hybrid SSE + HTTP arch..."
    },
    {
      "id": "doc-14",
      "title": "1. Record architecture decisions",
      "href": "/docs/decision-records/decisions/0001-record-architecture-decisions",
      "section": "Decision Records",
      "content": "Record architecture decisions\n\nStatus\n\nAccepted\n\nContext\n\nWe need to record the architectural decisions made on this project.\n\nDecision\n\nWe will use Architecture Decision Records, as described by Michael\nNygard.\n\nConsequences\n\nSee Michael Nygard's article, linked above. To create new ADRs we will copy and\npaste from existing ones.",
      "excerpt": "Record architecture decisions Status Accepted Context We need to record the architectural decisions made on this project. Decision We will use Architecture Decision Records, as described by Michael Ny..."
    },
    {
      "id": "doc-15",
      "title": "2. Adopt python-copier-template for project structure",
      "href": "/docs/decision-records/decisions/0002-switched-to-python-copier-template",
      "section": "Decision Records",
      "content": "Adopt python-copier-template for project structure\n\nStatus\n\nSuperseded by ADR-0011\n\nNote: This decision was reversed on 2026-01-05. See ADR-0011 for details.\n\nContext\n\nWe should use the following python-copier-template.\nThe template will ensure consistency in developer environments and package management.\n\nDecision\n\nWe have switched to using the template.\n\nConsequences\n\nThis module will use a fixed set of tools as developed in  and can pull from this\ntemplate to update the packaging to the latest techniques.\n\nAs such, the developer environment may have changed, the following could be different:\n\nlinting\nformatting\npip venv setup\nCI/CD",
      "excerpt": "Adopt python-copier-template for project structure Status Superseded by ADR-0011 Note: This decision was reversed on 2026-01-05. See ADR-0011 for details. Context We should use the following python-co..."
    },
    {
      "id": "doc-16",
      "title": "3. Short descriptive title",
      "href": "/docs/decision-records/decisions/0003-message-queue-message-grouping",
      "section": "Decision Records",
      "content": "Short descriptive title\n\nDate: 28/10/2024\n\nStatus\n\nIn Progress\n\nContext\n\nDo we want to have separate queues for different types of processing data feedback which will lead to\ndifferent method calls (i.e. CTF results vs. particle picking results)?\n\nThe other option would be to have the messages contain something to indicate the relevant callback method.\n\nDecision\n\nWhat decision we made.\n\nConsequences\n\nWhat we will do as a result of this decision.",
      "excerpt": "Short descriptive title Date: 28/10/2024 Status In Progress Context Do we want to have separate queues for different types of processing data feedback which will lead to different method calls (i.e. C..."
    },
    {
      "id": "doc-17",
      "title": "3. No dependency on python-zocalo",
      "href": "/docs/decision-records/decisions/0004-zocalo-dependency-free",
      "section": "Decision Records",
      "content": "No dependency on python-zocalo\n\nDate: 08/11/2024\n\nStatus\n\nAccepted\n\nContext\n\nWe initially needed to send logs to Graylog, and this functionality was already implemented in .\nHence, one option was to import any logging-related functionality from Zocalo, while another option was to just\ncopy the code across, duplicating it in our repo.\n\nUpdate: Graylog functionality has since been removed from the project. In production Kubernetes, all stdout and\nstderr are automatically sent to a managed Graylog instance, so we don't need to handle this in our application\ncode. For local development, we now use standard Python logging to keep things simple.\n\nDecision\n\nAccording to Dan Hatton:\n\nI was intending to keep it free of the zocalo dependency if possible because there are internal\ndiscussions about whether we move off of it longer term\n\nConsequences\n\nWe will stay free of any Zocalo dependencies. Initially we duplicated logging functionality within this project,\nbut have since simplified to use standard Python logging as production Kubernetes handles log aggregation\nautomatically.",
      "excerpt": "No dependency on python-zocalo Date: 08/11/2024 Status Accepted Context We initially needed to send logs to Graylog, and this functionality was already implemented in . Hence, one option was to import..."
    },
    {
      "id": "doc-18",
      "title": "5. Use detect-secrets for secret scanning",
      "href": "/docs/decision-records/decisions/0005-detect-secrets-for-secret-scanning",
      "section": "Decision Records",
      "content": "Use detect-secrets for secret scanning\n\nDate: 21/08/2025\n\nStatus\n\nAccepted\n\nContext\n\nThe SmartEM Decisions project requires robust secret scanning to protect sensitive research data, database\ncredentials, API keys, and Kubernetes cluster secrets. As part of the Diamond Light Source facility infrastructure,\nhigh security standards are essential whilst supporting scientific computing workflows.\n\nThe development team evaluated secret scanning tools for integration into the existing sophisticated pre-commit and\nCI/CD pipeline (Python 3.12+, ruff, pyright). The organisational cybersecurity team recommended Gitleaks for\nstandardisation across projects.\n\nKey requirements included:\nIntegration with Python 3.12+ ecosystem and existing toolchain\nHandling scientific computing patterns (chemical formulas, gene sequences, scientific notation) without excessive\n  false positives\nSupport for high-throughput processing without workflow disruption\nEnterprise-grade baseline management for research environments\n\nThree tools were evaluated:\nGitleaks: High-performance Go implementation, organisational preference, but higher false positives in\n  scientific contexts\nTruffleHog: Advanced entropy analysis, but resource-intensive with SaaS dependencies\ndetect-secrets: Python-native, superior false positive handling, sophisticated baseline management\n\nDecision\n\nWe will use detect-secrets as the primary secret scanning tool, integrated into both pre-commit hooks and\nCI/CD pipelines, despite the organisational preference for Gitleaks standardisation.\n\nConsequences\n\nPositive:\nNative Python integration with existing development workflow\nSuperior false positive management for scientific computing patterns\nEnterprise-grade baseline system for managing known safe patterns\nFaster CI/CD execution through incremental scanning approach\nFlexible plugin architecture for research-specific customisation\n\nNegative:\nDivergence from organisational tooling standardisation\nPotential knowledge silos between teams using different tools\nResponsibility for maintaining tool-specific expertise within the team\n\nMitigation:\nComprehensive documentation of configuration and workflows",
      "excerpt": "Use detect-secrets for secret scanning Date: 21/08/2025 Status Accepted Context The SmartEM Decisions project requires robust secret scanning to protect sensitive research data, database credentials,..."
    },
    {
      "id": "doc-19",
      "title": "6. Use Sealed Secrets for Kubernetes secrets management",
      "href": "/docs/decision-records/decisions/0006-sealed-secrets-kubernetes-secrets-management",
      "section": "Decision Records",
      "content": "Use Sealed Secrets for Kubernetes secrets management\n\nDate: 22/08/2025\n\nStatus\n\nAccepted\n\nContext\n\nThe SmartEM Decisions project requires secure secrets management for Kubernetes deployments across development, staging,\nand production environments. The current approach of committing plain secrets.yaml files with test passwords to version\ncontrol presents security risks and violates organisational security standards at Diamond Light Source.\n\nThe project deploys to three environments using local development machines without CI/CD automation. Secrets include\nPostgreSQL database credentials, RabbitMQ message queue authentication, and other sensitive configuration values\nrequired for the scientific computing workflow.\n\nThe Diamond Light Source Kubernetes team recommended Sealed Secrets for organisational standardisation, referencing\ntheir internal development portal guide (https://dev-portal.diamond.ac.uk/guide/kubernetes/tutorials/secrets/) for\nimplementation guidance.\n\nFive approaches were evaluated for secrets management:\n\nKustomize Secret Generators: Generate secrets at build time using kustomize secretGenerator functionality\nExternal Secrets Operator: Pull secrets from external systems such as HashiCorp Vault or AWS Secrets Manager\nSealed Secrets: Encrypt secrets using asymmetric cryptography that can be safely committed to version control\nEnvironment-Specific .env Files: Use gitignored .env files with generation scripts for each environment\nDevelopment Tools Integration: Generate secrets dynamically within the existing dev-k8s.sh deployment script\n\nKey requirements included:\nEliminate plain text secrets from version control\nSupport local deployment workflow without CI/CD infrastructure\nMaintain version control history for secret management\nAlign with Diamond Light Source organisational standards\nSupport secret rotation and environment-specific values\n\nDecision\n\nWe will use Sealed Secrets as the primary Kubernetes secrets management solution, implemented through the\nsealed-secrets controller and kubeseal CLI tool.\n\nSealed Secrets uses asymmetric cryptography where secrets are encrypted with a public key and can only be decrypted by\nthe sealed-secrets controller running in the target Kubernetes cluster. This allows encrypted SealedSecret resources to\nbe safely committed to version control whilst maintaining security.\n\nImplementation will include:\nInstallation of sealed-secrets controller in each Kubernetes environment\nIntegration with existing deployment workflow via tools/k8s/generate-sealed-secrets.sh script\nDocumentation in docs/how-to/manage-kubernetes-secrets.md for team procedures\nEnvironment-specific encryption keys for development, staging, and production clusters\n\nConsequences\n\nPositive:\nEliminates security risk of committing plain text secrets to version control\nAligns with Diamond Light Source organisational standards and team recommendations\nEnables version control tracking of secret changes whilst maintaining security\nSupports local deployment model without requiring external secret management infrastructure\nProvides secret rotation capabilities with audit trail through Git history\nIntegrates seamlessly with existing Kubernetes deployment workflow\n\nNegative:\nIntroduces learning curve for team members unfamiliar with sealed-secrets tooling\nCreates dependency on sealed-secrets controller availability in each cluster environment\nRequires careful management of sealed-secrets private keys for cluster recovery scenarios\nAdditional complexity compared to plain Kubernetes secrets for local development workflows\n\nImplementation considerations:\nBackup and secure storage of sealed-secrets private keys for disaster recovery\nDocumentation of secret rotation procedures for team knowledge sharing\nIntegration testing to ensure sealed-secrets work correctly across all three environments\nTraining for team members on kubeseal CLI usage and troubleshooting procedures",
      "excerpt": "Use Sealed Secrets for Kubernetes secrets management Date: 22/08/2025 Status Accepted Context The SmartEM Decisions project requires secure secrets management for Kubernetes deployments across develop..."
    },
    {
      "id": "doc-20",
      "title": "7. Eliminate circular dependency between smartem_api and smartem_backend",
      "href": "/docs/decision-records/decisions/0007-eliminate-smartem-api-circular-dependency",
      "section": "Decision Records",
      "content": "Eliminate circular dependency between smartemapi and smartembackend\n\nDate: 23/08/2025\n\nStatus\n\nAccepted\n\nContext\n\nThe SmartEM Decisions project developed a circular dependency between the  and  packages \nthat violated clean architecture principles and created maintenance challenges for the scientific computing workflow \nautomation system.\n\nThe circular dependency manifested in two ways:\nPackage-level dependency:  declared dependency on  in its pyproject.toml \n   configuration, requiring the API package for core backend functionality\nModule-level imports:  module imported extensively from  components \n   including database models, message queue publishers, and utility functions\n\nThis circular relationship created several problems for the cryo-electron microscopy workflow system:\nBuild complexity: Package installation order became critical and error-prone during development setup\nArchitectural violation: Backend logic was split across packages in non-intuitive ways, complicating system \n  understanding\nMaintenance burden: Changes to core backend functionality required coordinated updates across multiple packages\nImport confusion: Developers struggled to understand which package provided specific functionality\nTesting complexity: Circular imports made unit testing and mocking more difficult\n\nThe project's multi-package structure (, , , ) was designed \nto enable modular deployment and clear separation of concerns for the high-throughput microscopy data processing \nrequirements. However, the API package had evolved beyond its intended scope as a simple HTTP client interface.\n\nThree approaches were considered for resolving the circular dependency:\n\nDependency inversion: Introduce abstract interfaces to break direct import cycles whilst maintaining separate \n   packages\nPackage splitting: Further decompose packages to eliminate circular relationships through fine-grained separation\nPackage consolidation: Merge  functionality into  to eliminate the circular \n   dependency entirely\n\nDecision\n\nWe will consolidate the smartemapi package into smartembackend to eliminate the circular dependency completely.\n\nThe consolidation will restructure the codebase as follows:\nMove  to  for HTTP client functionality\nMove  to  for FastAPI server implementation\nRelocate HTTP data models to  alongside existing database models\nMaintain backward compatibility by creating re-export modules in the original  location\nRemove  as a separate package from pyproject.toml configuration\nIntroduce lightweight  dependency group for components requiring only API client functionality\n\nThe API client will remain accessible to  and external consumers through the consolidated package \nstructure, preserving the modularity required for distributed cryo-electron microscopy workflow processing.\n\nConsequences\n\nPositive:\nEliminates circular dependency completely, resolving architectural violation and build complexity\nReduces operational overhead by managing one fewer package in the development and deployment pipeline\nSimplifies import structure with clear single-source responsibility for HTTP API functionality\nMaintains full backward compatibility for existing code using  imports\nPreserves API client reusability for  and external scientific computing integrations\nContinues to support Swagger documentation generation from the consolidated API server\nEnables cleaner dependency management with optional  group for minimal installations\n\nNegative:\nIncreases  package scope, potentially affecting cognitive load for developers\nRequires migration period where both import paths coexist, creating temporary code duplication\nMay impact deployment strategies that relied on separate API package for containerised microservices\nDocumentation and examples require updates to reflect new package structure\n\nImplementation considerations:\nComprehensive testing across all import paths to ensure backward compatibility during transition period\nDocumentation updates for installation guides, API usage examples, and architectural diagrams\nCommunication to development team about new import paths and deprecation timeline for legacy imports\nValidation of Swagger documentation generation and API client functionality post-consolidation",
      "excerpt": "Eliminate circular dependency between smartemapi and smartembackend Date: 23/08/2025 Status Accepted Context The SmartEM Decisions project developed a circular dependency between the and packages that..."
    },
    {
      "id": "doc-21",
      "title": "8. Backend-to-Agent Communication Protocol Selection",
      "href": "/docs/decision-records/decisions/0008-backend-to-agent-communication-architecture",
      "section": "Decision Records",
      "content": "Backend-to-Agent Communication Protocol Selection\n\nDate: 26/08/2025\n\nStatus\n\nAccepted\n\nContext\n\nThe SmartEM Decisions system requires a communication protocol for delivering real-time microscope control instructions from Kubernetes-hosted backend services to Windows workstation agents controlling cryo-electron microscopes. \n\nKey Requirements\nReal-time delivery: Instructions must reach agents within seconds of generation\nHigh throughput: Support high-frequency microscopy workflows  \nFault tolerance: Connection failures must not result in lost instructions\nNetwork compatibility: Must work within existing network infrastructure\nIntegration: Seamless integration with existing FastAPI and RabbitMQ architecture\n\nConstraints\nWindows workstations have limited network connectivity\nAgents cannot initiate connections to backend (firewall restrictions)\nInstructions occur every 30-120 seconds per agent during data collection\nScientific reproducibility requires reliable instruction delivery and audit trails\n\nDecision\n\nWe will implement Server-Sent Events (SSE) for instruction streaming combined with HTTP POST acknowledgements.\n\nAlternatives Considered\n\nWebSockets\nPros: Bidirectional, low latency, excellent real-time performance\nCons: Complex state management, firewall compatibility issues, unnecessary complexity for 30-120 second instruction intervals, fragile in environments with routine network maintenance\nVerdict: Unsuitable - over-engineered for microscope control timing patterns\n\nHTTP Long-Polling  \nPros: Simple HTTP-based, good firewall compatibility, natural timeout handling\nCons: Resource intensive with blocking connections, complex timeout management, potential connection exhaustion\nVerdict: Rejected - inefficient resource usage\n\ngRPC Streaming\nPros: Excellent performance, built-in streaming, strong typing\nCons: HTTP/2 proxy compatibility issues, Protocol Buffers complexity, firewall restrictions, over-engineered for infrequent instructions\nVerdict: Rejected - unnecessary complexity\n\nMessage Queue Pull (Direct RabbitMQ)\nPros: Native reliability, mature authentication, built-in failover\nCons: Exposes message queue to restricted networks, complex credential management, conflicts with network isolation policies\nVerdict: Rejected - security boundary violations\n\nFile-Based Communication\nPros: Simple implementation, natural persistence, no network dependencies\nCons: Polling overhead, poor real-time performance, file locking complexity, inadequate for sub-second requirements\nVerdict: Rejected - insufficient performance\n\nConsequences\n\nPositive\nOptimal performance: SSE provides real-time delivery with minimal latency for microscopy workflows\nReliable delivery: HTTP acknowledgements ensure instruction receipt confirmation\nFault tolerance: Automatic fallback to HTTP polling during connection issues\nNetwork compatibility: HTTP/SSE protocols work within existing firewall configurations\nSimple integration: Leverages existing FastAPI infrastructure with minimal changes\nAudit compliance: HTTP-based acknowledgements provide full instruction traceability\n\nNegative  \nConnection management: SSE connections require careful lifecycle management with retry logic\nProxy sensitivity: Long-lived SSE connections may be affected by corporate proxies\nRetry complexity: Exponential backoff logic must handle various failure scenarios\n\nPolling Fallback Rejected\n\nInitial consideration included HTTP polling as a fallback mechanism, but this was rejected for the following reasons:\n\nComplexity without benefit: Network issues affecting SSE typically also affect HTTP polling\nInstruction frequency: 30-120 second intervals make polling overhead unnecessary  \nRobust retry sufficient: SSE with exponential backoff reconnection handles temporary failures\nFailure correlation: Most network problems (DNS, firewall, proxy) impact both protocols equally\nMaintenance burden: Dual code paths increase complexity without meaningful reliability improvement\n\nDecision: Implement robust SSE with retry logic only, without polling fallback.\n\nImplementation Requirements\nFastAPI SSE endpoint with connection lifecycle management and retry logic\nHTTP acknowledgement endpoints for delivery confirmation  \nExponential backoff reconnection for SSE failures\nIntegration with existing RabbitMQ event system\nDatabase persistence for instruction state and delivery tracking",
      "excerpt": "Backend-to-Agent Communication Protocol Selection Date: 26/08/2025 Status Accepted Context The SmartEM Decisions system requires a communication protocol for delivering real-time microscope control in..."
    },
    {
      "id": "doc-22",
      "title": "ADR 0009: Commit Generated Route Tree to Version Control",
      "href": "/docs/decision-records/decisions/0009-commit-generated-route-tree",
      "section": "Decision Records",
      "content": "ADR 0009: Commit Generated Route Tree to Version Control\n\nStatus\n\nAccepted\n\nApplies To\n\nsmartem-frontend\n\nContext\n\nTanStack Router uses file-based routing and generates a  file that contains the complete route tree for type safety. This generated file needs to be kept in sync with the route files, but there are different approaches to managing generated files:\n\nCommit the file: Treat it as source code and commit to version control\nGitignore the file: Add to  and regenerate on every build\nHybrid approach: Commit but use CI checks to verify it's up to date\n\nDifferent tools in the ecosystem handle this differently:\n\nGraphQL Codegen: Typically gitignored, regenerated via pre-scripts\nPrisma Client: Typically gitignored, regenerated on install\nTanStack Router: TanStack officially recommends committing the file\n\nThe key consideration is that  provides TypeScript types that are needed immediately when developers clone the repository or switch branches.\n\nDecision\n\nWe will commit  to version control and add CI verification to ensure it stays in sync with route files.\n\nImplementation\n\nCommit the generated file: Include  in version control\nConfigure linter to ignore it: Add to biome  to skip linting/formatting\nAdd verification scripts:\n\nCI verification: Add  to CI pipeline to fail if file is out of sync\nInstall CLI as devDependency: Add  to ensure consistent versions\n\nNo Git Hooks\n\nWe explicitly chose not to use git hooks (pre-commit/pre-push) because:\n\nThe Vite plugin already auto-generates during development\nGit hooks would create redundant generation\nCan cause confusion with \"double generation\" issues\nSlows down commit process unnecessarily\n\nConsequences\n\nPositive\n\nType safety on clone: Developers get TypeScript types immediately after cloning\nWorks in CI: Type checking works without needing generation step before \nBranch switching: Types are correct immediately when switching branches\nClear audit trail: Changes to routes are visible in git diffs\nFollows official guidance: Aligned with TanStack Router's recommendation\n\nNegative\n\nMerge conflicts: Generated file can cause conflicts when multiple people modify routes\n  - Mitigation: When conflicts occur, regenerate with  rather than manually resolving\nRepository size: Adds ~14KB generated file to repository\n  - Impact: Negligible for most projects\nCI dependency: Requires CI to run verification check\n  - Implementation: Simple check: \n\nNeutral\n\nDeveloper workflow: No change - Vite plugin handles generation automatically during development\nManual sync rarely needed: Only if someone edits routes without running dev server (rare)\n\nAlternatives Considered\n\nAlternative 1: Gitignore and Regenerate\n\nApproach: Add  to  and regenerate on every build\n\nRejected because:\n\nType checking fails immediately after clone\nCI needs extra generation step before \nCreates \"works on my machine\" issues if generation differs\nAgainst TanStack Router's official recommendation\n\nAlternative 2: Git Hooks\n\nApproach: Use pre-commit hooks to regenerate before each commit\n\nRejected because:\n\nVite plugin already handles generation during dev\nCreates redundant/duplicate generation\nSlows down commits\nCan cause confusion about which generation is \"correct\"\n\nReferences\n\nTanStack Router FAQ: Should I commit routeTree.gen.ts?\nTanStack Router Discussion #1218\nFile-Based Routing Documentation\n\nDate\n\n2025-01-12",
      "excerpt": "ADR 0009: Commit Generated Route Tree to Version Control Status Accepted Applies To smartem-frontend Context TanStack Router uses file-based routing and generates a file that contains the complete rou..."
    },
    {
      "id": "doc-23",
      "title": "11. Remove python-copier-template",
      "href": "/docs/decision-records/decisions/0011-remove-python-copier-template",
      "section": "Decision Records",
      "content": "Remove python-copier-template\n\nStatus\n\nAccepted\n\nContext\n\nIn ADR-0002, we adopted the python-copier-template to ensure consistency in developer environments and package management.\n\nSince that decision, the smartem-decisions project has evolved significantly:\n\nProject maturity: The project has grown from a single-package PoC to a multi-package monorepo with custom requirements that diverge from standard DLS Python projects\nCustom tooling needs: Our development workflow now requires tooling configurations specific to our architecture (multi-package structure, agent deployment, Kubernetes manifests, RabbitMQ integration)\nTemplate update friction: The copier template's update mechanism became a maintenance burden rather than a benefit, as most updates were not relevant to our custom structure\nDuplicated documentation: The template's contribution guidelines and developer documentation conflicted with our own evolving practices documented in smartem-devtools\n\nDecision\n\nWe have removed the python-copier-template dependency and scaffolding from smartem-decisions.\n\nThe following were removed in commit f95b1de (2026-01-05):\nconfiguration file\nCopier dependency from \nTemplate-generated sections from \n\nWe retain the tooling standards established by the template (pyright, ruff, pre-commit) but now manage their configuration directly.\n\nConsequences\n\nPositive\n\nReduced maintenance burden: No need to resolve conflicts when updating from the template\nCustom workflows: Freedom to evolve tooling and structure to match our specific needs\nSimplified onboarding: Developer documentation is now solely in smartem-devtools, not split between template and repo\nClearer ownership: All configuration is explicitly managed by the team\n\nNegative\n\nManual updates: We no longer automatically receive updates to best practices from the template\nDivergence risk: May drift from DLS Python conventions over time\nResponsibility: Must actively maintain tooling standards ourselves\n\nMitigations\n\nContinue following DLS Python best practices where applicable\nReference the copier template repo for inspiration when updating tooling\nDocument our standards explicitly in smartem-devtools\nMaintain pre-commit hooks to enforce code quality standards\n\nReferences\n\nADR-0002: Adopt python-copier-template (superseded by this decision)\nRemoval commit: f95b1dea1479d8d845f5cfd605084c201f459020\nDLS python-copier-template: https://github.com/DiamondLightSource/python-copier-template",
      "excerpt": "Remove python-copier-template Status Accepted Context In ADR-0002, we adopted the python-copier-template to ensure consistency in developer environments and package management. Since that decision, th..."
    },
    {
      "id": "doc-24",
      "title": "12. Audience-Based Documentation Structure",
      "href": "/docs/decision-records/decisions/0012-audience-based-documentation-structure",
      "section": "Decision Records",
      "content": "Audience-Based Documentation Structure\n\nDate: 2026-01-06\n\nStatus\n\nAccepted\n\nContext\n\nThe SmartEM documentation inherited a Diataxis framework (Tutorials, How-To, Explanations, Reference) from the Python Copier template. While Diataxis is well-regarded for general-purpose documentation, it presents challenges for our specific context:\n\nProblems with Diataxis for SmartEM\n\nAudience ambiguity: A \"How-To\" guide doesn't indicate whether it's for developers, operators, or integrators. Users must read content to determine relevance.\n\nComponent fragmentation: Information about the Agent is scattered across how-to guides, explanations, and reference sections, requiring navigation across multiple directories.\n\nUse-case disconnect: Common tasks like \"deploy to production\" or \"debug agent issues\" span multiple Diataxis categories, creating friction for users with specific goals.\n\nGrowing complexity: As the system expands to multiple components (Backend, Agent, Athena, Frontend), generic categories become insufficient.\n\nRequirements\n\nClear entry points for different audiences (developers, operators, integrators)\nComponent-focused documentation that groups related information\nUse-case driven organisation that matches how users approach the system\nScalable structure as new components are added\n\nDecision\n\nWe will restructure documentation around three axes:\n\nAudience: Developers, Operators, Integrators\nComponent: Backend, Agent, Athena (and future components)\nUse-Case: Getting Started, Operations, Development, Architecture\n\nTarget Structure\n\nStructure Principles\n\nTop-level directories reflect components and cross-cutting concerns\nEach section includes audience indicators where content differs by role\nADRs remain in  as the authoritative location\nAPI reference docs (Swagger/OpenAPI) remain generated and auto-published\n\nConsequences\n\nPositive\n\nClear navigation: Users can quickly find content relevant to their role\nComponent coherence: All Agent docs in one place, all Backend docs together\nScalable: New components get new directories without restructuring\nReduced duplication: Shared concepts documented once, linked from context\n\nNegative\n\nMigration effort: Existing docs must be reorganised and some rewritten\nLink breakage: External links to old structure will break (redirects needed)\nLearning curve: Contributors must understand new structure\n\nMigration Notes\n\nExisting content will be reorganised, not discarded\nSphinx configuration will be updated for new structure\nWebUI will require corresponding updates (separate task)",
      "excerpt": "Audience-Based Documentation Structure Date: 2026-01-06 Status Accepted Context The SmartEM documentation inherited a Diataxis framework (Tutorials, How-To, Explanations, Reference) from the Python Co..."
    },
    {
      "id": "doc-25",
      "title": "13. EPUPlayer Release Strategy",
      "href": "/docs/decision-records/decisions/0013-epuplayer-release-strategy",
      "section": "Decision Records",
      "content": "EPUPlayer Release Strategy\n\nDate: 2026-01-20\n\nStatus\n\nAccepted\n\nContext\n\nEPUPlayer is a filesystem recording and replay tool used for development and testing of the SmartEM system. It records filesystem changes (file creations, modifications, deletions) and can replay them with configurable timing - essential for testing components that react to filesystem events without requiring real microscope sessions.\n\nThe tool serves two distinct audiences with different requirements:\n\nWindows users (EPU workstation operators, facility staff): Need a standalone executable that can run without Python installation. These users operate microscope workstations and need to record EPU filesystem activity.\n\nPython developers (SmartEM contributors, CI/CD pipelines): Need a pip-installable package for integration into development workflows, testing pipelines, and programmatic usage.\n\nPreviously, EPUPlayer existed as a single script in  with a manual build process for Windows executables. This created challenges:\n\nNo permanent URLs for Windows releases (artifacts expired)\nNo PyPI package for pip installation\nNo automated release process\nNo version tracking\nMonorepo structure required path-based change detection for releases\n\nDecision\n\nWe will implement a dual-distribution release strategy for EPUPlayer:\n\nPackage Location\n\nMove EPUPlayer from  to  as a proper Python package with:\nusing hatchling build backend\nModular code structure (models, recorder, replayer, cli)\nEntry point:  command\nPackage name: \n\nVersion Strategy\n\nUse semantic versioning starting at v1.0.0\nTag format:  (e.g., )\nRC releases:  (e.g., )\n\nRelease Triggers\n\n Condition \n\n Always \n  changed \n  changed \n\nDistribution Channels\n\nGitHub Releases (both stable and RC):\n   - Windows executable: \n   - Python wheel: \n   - Source distribution: \n   - Permanent URLs, never expire\n\nPyPI (stable releases only):\n   - Package: \n   - Uses Trusted Publishers (OIDC) for secure publishing\n   - RC releases excluded to keep PyPI clean\n\nRelease Notes\n\nHybrid approach for monorepo context:\nQuery PRs with  label merged since last tag\nFallback to git log with path filter for any missed commits\nAuto-labeling via  workflow using \n\nBuild Process\n\nWindows exe: PyInstaller with watchdog hidden imports\nPython package: hatchling via uv build\nSmoke tests: CLI help, record, info, and replay commands\n\nConsequences\n\nPositive\n\nPermanent URLs: Windows users can always download specific versions\nPyPI discoverability: Python developers can \nClean separation: Package structure supports future growth\nAutomated releases: No manual steps required\nRC testing: Changes merged to main are automatically released as RCs for testing\nMonorepo-aware: Only triggers when package files change\n\nNegative\n\nSetup required: Need to configure PyPI Trusted Publishers\nProject token: PR automation needs  secret for project board access\nBreaking change: Old  path no longer exists\n\nNeutral\n\nOld Windows artifacts will remain in GitHub Actions (90-day retention)\nExisting recordings remain compatible (format unchanged)",
      "excerpt": "EPUPlayer Release Strategy Date: 2026-01-20 Status Accepted Context EPUPlayer is a filesystem recording and replay tool used for development and testing of the SmartEM system. It records filesystem ch..."
    },
    {
      "id": "doc-26",
      "title": "Architectural Decision Records",
      "href": "/docs/decision-records/decisions",
      "section": "Decision Records",
      "content": "Architectural Decision Records\n\nArchitectural decisions are made throughout a project's lifetime. As a way of keeping track of these decisions, we record these decisions in Architecture Decision Records (ADRs) listed below.\n\nADRs\n\nADR-0001: Record Architecture Decisions\nADR-0002: Switched to Python Copier Template (superseded by ADR-0011)\nADR-0003: Message Queue Message Grouping\nADR-0004: Zocalo Dependency-Free\nADR-0005: Detect Secrets for Secret Scanning\nADR-0006: Sealed Secrets for Kubernetes\nADR-0007: Eliminate SmartEM API Circular Dependency\nADR-0008: Backend to Agent Communication Architecture\nADR-0009: Commit Generated Route Tree\nADR-0010: Use Shiki for Syntax Highlighting\nADR-0011: Remove Python Copier Template\n\nFor more information on ADRs see this blog by Michael Nygard.",
      "excerpt": "Architectural Decision Records Architectural decisions are made throughout a project's lifetime. As a way of keeping track of these decisions, we record these decisions in Architecture Decision Record..."
    },
    {
      "id": "doc-27",
      "title": "EPU Output Directory Structure",
      "href": "/docs/decision-records/epu-data-structures",
      "section": "Decision Records",
      "content": "EPU Output Directory Structure\n\nDirectory layout on the EPU machine will differ from the layout of that\nsame directory synced to file storage. Layout relevant to epu data intake component\nis as it appears on the EPU machines, where the watcher runs.\n\nIn a user visit there can be more than one  and more than one \"project dir\"\nas there will be one for each grid. We usually treat each  independently,\nbut they all connect to the same atlas.\n\nEPU Directory Structure Details\n\nAn EPU directory consists of multiple subdirectories as well as ,  and  files\n  - Any  files are actually XML files\n  - This directory is written to incrementally by EPU software, it does not materialise in a complete state right away\nAn EPU directory will contain a file named  at root level. Useful information contained in this file:\n  - Session name, id and start time;\n  - fs path of the EPU directory;\n  - A reference to  file (which resides outside the EPU directory);\n  - clustering mode and radius\nAn  file contains general information about the acquisition session (which possibly duplicates info already\n  found in  and, crucially, information about atlas tiles\n  - Atlas tiles contain positioning information, which is needed to map physical positions on the grid (measured in\n    number of turns of the actuator) to their pixel coordinates in image files\nAn EPU directory will contain a subdirectory named  at root level\n  -  directory is flat and contains a large list of files all using a naming convention\n    , for example: , \n  - The  directory contains all grid squares not just the ones that are captured\n  - Some  files will have a significantly larger file size than others.\n    Those are the ones that were of interest and so further scans took place during the acquisition session\n  - The rest of GridSquare files under  are typically around 2.8Kb in size\nAn EPU directory contains at least one subdirectory named  at root level,\n  and it's possible to have multiple subdirectories with a naming convention , though in most\n  cases there will only be one.\n  - An  directory will contain a number of subdirectories following a naming convention\n    , corresponding to GridSquare files in , but only for GridSquares of\n    interest where further scanning took place\nA GridSquare directory matching a glob  will contain:\n  - a GridSquare manifest file, such as \n  - optionally subdirectories  and , containing FoilHole and Micrograph information, respectively",
      "excerpt": "EPU Output Directory Structure Directory layout on the EPU machine will differ from the layout of that same directory synced to file storage. Layout relevant to epu data intake component is as it appe..."
    },
    {
      "id": "doc-28",
      "title": "Decision Records",
      "href": "/docs/decision-records",
      "section": "Decision Records",
      "content": "Decision Records\n\nThis section contains architectural decision records (ADRs) and design documentation for SmartEM.\n\nArchitecture Decision Records\n\nADRs document significant technical decisions made during development. See ADR-0001 for the decision to use this format.\n\nAll ADRs - Complete list of architecture decision records\n\nDesign Documents\n\nBackend-Agent Communication - System design for SSE-based communication\nSmartEM Agent Design - Agent architecture and EPU integration\nEPU Data Structures - EPU XML schemas and data models\nTechnical Notes - Implementation notes and research",
      "excerpt": "Decision Records This section contains architectural decision records (ADRs) and design documentation for SmartEM. Architecture Decision Records ADRs document significant technical decisions made duri..."
    },
    {
      "id": "doc-29",
      "title": "SmartEM Agent: Design Specification",
      "href": "/docs/decision-records/smartem-agent-design",
      "section": "Decision Records",
      "content": "SmartEM Agent: Design Specification\n\nVersion: 2.0\nDate: 28/10/2025\nStatus: Implemented\n\nExecutive Summary\n\nThis document specifies the design for the SmartEM Agent, the real-time EPU filesystem monitoring service. The\nimplementation addresses non-deterministic file ordering issues that previously caused GridSquare and FoilHole\nprocessing failures. The agent provides robust orphan handling for arbitrary entity relationship ordering, improved\nbursty write handling, and enhanced code maintainability whilst preserving the proven parser implementation.\n\nBackground\n\nImplementation Overview\n\nSmartEM Agent is a Windows-deployed service that monitors EPU (cryo-electron microscopy software) output directories\nin real time, parsing XML metadata and synchronising entity data to the backend via REST API. The architecture\n() comprises several key components:\n\nParser (): XML parsing of EPU session manifests, atlas data, GridSquare metadata, FoilHole\n  manifests, and micrograph metadata\nEvent Classifier (): Classifies file events by entity type and assigns processing priority\nEvent Queue (): Priority queue for buffering classified events during bursty writes\nEvent Processor (): Coordinates parsing, parent checking, and orphan management\nOrphan Manager (): Manages entities awaiting parent availability with event-driven resolution\nError Handler (): Categorises and handles transient vs permanent errors with retry logic\nWatcher (): Filesystem monitoring using watchdog library, orchestrating all components\nData Store (): In-memory entity cache with optional persistent backend synchronisation via\n  . Provides relationship tracking and natural ID lookups for deduplication\n\nThe agent operates in two modes:\nDry-run mode: In-memory data store only, no API persistence\nProduction mode: Persistent data store with REST API synchronisation and SSE instruction streaming\n\nDeployment occurs as a Windows executable, with testing facilitated by epuplayer playback simulation of EPU output\npatterns.\n\nHistorical Problem Statement\n\nThe original implementation exhibited critical failures when processing EPU filesystem output, manifesting primarily in\nGridSquare and FoilHole entity processing during end-to-end tests. These failures stemmed from three core architectural\ndeficiencies that have now been addressed:\n\nInsufficient Orphan Handling (Critical Priority)\n\nEPU filesystem output exhibits completely non-deterministic ordering characteristics:\n\nChild-before-parent sequences: FoilHole manifests can appear before their parent GridSquare metadata files;\n  similarly, Micrographs can precede FoilHoles, and GridSquares can precede Atlas data.\nBursty file writes: EPU buffers and dumps hundreds of files simultaneously in unpredictable sequences.\nNo ordering guarantees: The filesystem provides no guarantees about write completion order, even for logically\n  related entities.\n\nThe current orphan handling implementation ( method, lines 527-543 in )\ncontains several weaknesses:\n\nGrid-level orphan processing only: Orphans are processed exclusively when new grids are detected. GridSquare and\n  FoilHole orphans that arrive before their parents are not handled.\nSingle retry attempt: Orphaned files are processed once when a potential parent appears, with no mechanism for\n  subsequent retry if the parent-child chain remains incomplete.\nPath-based resolution: Relies on  for orphan-to-grid matching, which fails for entities\n  requiring UUID-based parent lookups (GridSquares needing Grid UUIDs, FoilHoles needing GridSquare UUIDs).\nNo persistent orphan state: Orphaned entities are stored only in the  dictionary without timeout\n  tracking or systematic retry scheduling.\n\nThis results in silent processing failures for entities arriving in unexpected orders, particularly evident in\nend-to-end test failures where GridSquares and FoilHoles fail to persist to the database.\n\nInadequate Bursty Write Handling\n\nEPU can generate hundreds of files within seconds during data collection bursts. The current rate-limiting approach\n( parameter, default 10 seconds) provides only event batching for logging purposes, not processing\ncontrol. Issues include:\n\nSerial processing bottleneck: All events process sequentially in , creating backlog during bursts.\nNo backpressure mechanism: High-frequency events accumulate in  dictionary without processing\n  limits.\nInsufficient buffering strategy: Event accumulation occurs but processing remains unbounded and\n  non-prioritised.\n\nThe optimal processing model (serial versus concurrent) requires investigation. Entity interdependencies suggest serial\nprocessing within entity hierarchies (Grid → GridSquare → FoilHole → Micrograph) may be necessary to maintain\nparent-child ordering guarantees, whilst cross-hierarchy processing could potentially parallelise.\n\nCode Maintainability Concerns\n\nThe current watcher implementation exhibits technical debt that impedes modification and testing:\n\nComplex conditional nesting: The  method (lines 441-515) contains d",
      "excerpt": "SmartEM Agent: Design Specification Version: 2.0 Date: 28/10/2025 Status: Implemented Executive Summary This document specifies the design for the SmartEM Agent, the real-time EPU filesystem monitorin..."
    },
    {
      "id": "doc-30",
      "title": "Developer Guide: smartem-workspace",
      "href": "/docs/decision-records/smartem-workspace-developer-guide",
      "section": "Decision Records",
      "content": "Developer Guide: smartem-workspace\n\nThis guide is for developers who want to contribute to or modify the  package.\n\nTable of Contents\n\nArchitecture Overview\nPackage Structure\nCore Components\nConfiguration System\nDevelopment Setup\nTesting Strategy\nCI/CD Pipeline\nContributing Guidelines\nTroubleshooting Development\nAPI Reference\nExtending the Tool\n\nArchitecture Overview\n\nDesign Philosophy\n\n follows these principles:\n\nNetwork-first configuration - Fetch latest repository metadata from GitHub, fall back to bundled config\nZero permanent installation - Designed for  (run without install)\nIdempotent operations - Safe to re-run without side effects\nProgressive enhancement - Core functionality works offline with graceful degradation\nExplicit over implicit - Clear user prompts, no hidden magic\nFail fast - Validate early, provide actionable error messages\n\nSystem Design\n\nData Flow\n\nConfiguration Loading Flow\n\nRepository Cloning Flow\n\nError Handling Strategy\n\nValidation errors: Fail immediately with clear message (e.g., invalid preset name)\nNetwork errors: Degrade gracefully (use bundled config)\nGit errors: Log and continue for individual repos, fail if all repos fail\nPermission errors: Fail immediately with actionable guidance\nUser cancellation: Clean up partial state, exit gracefully\n\nPackage Structure\n\nModule Responsibilities\n\n Purpose \n\n CLI entry point \n Load configuration \n Data models \n User prompts \n Orchestration \n Git operations \n Claude Code \n Serena MCP \n Workspace files \n Git helpers \n Path utilities \n\nCore Components\n\nCLI Interface (cli.py)\n\nBuilt with Typer for type-safe CLI parsing.\n\nKey Design Decisions\n\nRich output: Uses  library for beautiful terminal output\nSubcommands: Each command (, , , ) is a separate function\nType safety: All arguments are type-hinted, Typer validates at runtime\nHelp text: Extensive help strings auto-generate documentation\n\nCommand Structure\n\nExit Codes\n\n Meaning ------\n Success  1 \n User cancelled  3 \n Git operation failed  Field  Description --------------------   Config schema version (for future migrations)    Unique identifier (used in presets)    GitHub organisation or GitLab group    Repository name (matches GitHub)    HTTPS clone URL    Shown in selection menu     (can edit),  (read-only),  (DLS mirror of external)    ERIC work package deliverable    ERIC WP number (or null)    Unique identifier    , , or     Source path in repository    Target path in workspace    Display name for UI    Short description    Repository IDs to include  Module  Notes ---------------   Tested via integration tests    Network fetching not mocked    Pydantic validation tests    Orchestration tested end-to-end    Git operations tested manually    Symlink creation tested manually    Helper functions unit tested    Path utilities unit tested  Event  Jobs Run ----------------- Pull Request  test, lint, build  Push to main  test, lint, build, version-bump, publish-testpypi  Release tag  test, lint, build, publish-pypi  Manual  test, lint, build  Commit Prefix  Example ------------------------   0.1.0 → 0.2.0    0.1.0 → 0.1.1    0.1.0 → 1.0.0  , , etc.  0.1.0 → 0.1.0 |\n\nExample commit messages:\n\nEnvironments\n\nGitHub Environments configured for publishing:\n\ntestpypi\nURL: https://test.pypi.org/p/smartem-workspace\nSecret: \nProtection: None (auto-deploy on main)\n\npypi\nURL: https://pypi.org/p/smartem-workspace\nSecret: \nProtection: Require manual approval (optional)\n\nContributing Guidelines\n\nCode Standards\n\nLine length: 120 characters (configured in ruff)\nType hints: Required for all function signatures\nDocstrings: Google style for public functions\nNo emojis: Windows compatibility (avoid Unicode in code/comments)\nBritish English: For documentation (e.g., \"organise\", not \"organize\")\n\nCommit Message Format\n\nFollow Conventional Commits:\n\nTypes:\n: New feature\n: Bug fix\n: Documentation changes\n: Maintenance tasks\n: Test additions/changes\n: Code restructuring\n: CI/CD changes\n\nExamples:\n\nPR Workflow\n\nFork or branch: Create feature branch from \nImplement changes: Code + tests + documentation\nRun checks: \nCommit: Use conventional commit format\nPush: \nCreate PR: Use GitHub PR template\nAddress feedback: Respond to review comments\nMerge: Squash and merge to main\n\nCode Review Checklist\n\n[ ] Tests added for new functionality\n[ ] Documentation updated (README, user guide, or developer guide)\n[ ] Code follows style guide (ruff passes)\n[ ] Type hints on all functions\n[ ] No emojis in code\n[ ] Commit messages follow conventional format\n[ ] PR description explains the why, not just the what\n\nRelease Process\n\nMerge PRs to main: CI auto-bumps version based on commits\nVerify TestPyPI: Check https://test.pypi.org/project/smartem-workspace/\nCreate release: Tag format: \n\nCI publishes to PyPI: Automatic on tag push\nVerify production: \nCreate GitHub Release: Add release notes\n\nTroubleshooting Development\n\nImport Errors\n\nError: \n\nSolutions:\n\nTest Failures\n\nError: Tests fail locally but pass in CI\n\nCommon causes:\nPython version mism",
      "excerpt": "Developer Guide: smartem-workspace This guide is for developers who want to contribute to or modify the package. Table of Contents Architecture Overview Package Structure Core Components Configuration..."
    },
    {
      "id": "doc-31",
      "title": "Technical Notes",
      "href": "/docs/decision-records/technical-notes",
      "section": "Decision Records",
      "content": "Technical Notes\n\nCryo-EM Data Acquisition Scale\n\nUnderstanding the scale and volume of cryo-electron microscopy data acquisition is crucial for system design and performance considerations:\n\nMicrograph Statistics\nMicrographs per foil hole: Typically 4-10 images per foil hole\nGrid-level acquisition: 10,000-50,000 micrographs per complete grid session\nParticle density: Approximately 300 particles identified per micrograph\nSelection efficiency: Roughly 50% of identified particles are selected for further processing\n\nThese statistics inform the design requirements for data processing pipelines and storage allocation strategies.\n\nSystem Architecture Considerations\n\nDecision-Making Framework\nThe SmartEM system implements a modular decision-making architecture designed for flexibility and extensibility:\n\nModular design: Decision-making components are decoupled from data acquisition systems\nPluggable authorities: Different decision-making algorithms can be easily substituted\nFuture extensibility: Architecture supports integration of additional decision-making systems\n\nAPI Integration\nCommunication between SmartEM components and external cryo-EM control systems occurs through well-defined API interfaces, ensuring compatibility with various microscope control software packages.\n\nData Management and Storage\n\nISPyB Integration\nThe project integrates with the ISPyB database schema, which provides comprehensive metadata storage for:\n\nExperimental run information\nSession metadata and parameters\nImage counts and acquisition statistics\nSample type and classification data\n\nFile System Organisation\nData acquisition produces structured file system layouts that facilitate automated processing and quality assessment workflows.\n\nProcessing Pipeline Components\n\nParticle Processing Workflow\nThe automated processing pipeline comprises several specialised services:\n\nParticle Picking Service\nInput: JSON message via RabbitMQ containing image path and processing parameters\nProcessing: Automated particle identification and coordinate extraction\nOutput: List of particle coordinates for each processed micrograph\nImplementation: CryOLO service\n\nParticle Selection Service\nFunction: Quality-based filtering and selection of identified particles\nIntegration: Seamless integration with particle picking results\nImplementation: Selection service\n\nScientific Context and References\n\nThe SmartEM system builds upon established methodologies and practices in automated cryo-electron microscopy:\n\nKey Publications\nStructural Biology Methods - Comprehensive overview of automated cryo-EM data collection strategies\nAdvanced Automation Techniques - Recent developments in real-time decision making for cryo-EM workflows\n\nThese references provide scientific context for the automated decision-making algorithms implemented within the SmartEM framework.",
      "excerpt": "Technical Notes Cryo-EM Data Acquisition Scale Understanding the scale and volume of cryo-electron microscopy data acquisition is crucial for system design and performance considerations: Micrograph S..."
    },
    {
      "id": "doc-32",
      "title": "Contributing",
      "href": "/docs/development/contributing",
      "section": "Development",
      "content": "",
      "excerpt": ""
    },
    {
      "id": "doc-33",
      "title": "End-to-End Development Test Runs",
      "href": "/docs/development/e2e-simulation",
      "section": "Development",
      "content": "End-to-End Development Test Runs\n\nThis document describes the complete workflow for running repeatable end-to-end tests of the SmartEM system using pre-recorded microscope sessions. It serves as both a runbook for executing tests and a reference for understanding test requirements.\n\nQuick Start (Automated Test Runner)\n\nFor the simplest test execution, use the automated test runner script:\n\nScript parameters (all optional):\nRecording file path (default: )\nEPU output directory (default: )\nMax delay in seconds (default: )\n\nEnvironment variable overrides:\n: Override default recording path\n: Override default EPU directory\n\nWorkspace structure: Scripts assume a multi-repo workspace layout where this repo lives at .\n\nWhat the script does:\nCreates timestamped test results directory in \nActivates venv and loads environment variables from \nResets database to clean state\nStarts API server and consumer\nStarts agent watching empty directory\nRuns playback with specified timing\nCollects filesystem and database statistics\nSaves all logs to test results directory\nCleans up background processes on exit\n\nWhen to use:\nQuick smoke tests during development\nConsistent repeatable test runs\nPre-Acquisition scenario testing (Test Type 2)\n\nWhen to use manual execution instead:\nTesting Post-Acquisition scenario (Test Type 1)\nTesting Mid-Acquisition scenario (Test Type 3)\nDebugging specific service interactions\nCustom timing or service startup order\n\nQuick Start (Manual Execution)\n\nIf you've already set up once and just need to run another test:\n\nFor detailed explanations, first-time setup, or different test scenarios, see sections below.\n\nMulti-Microscope Test (Concurrent Sessions)\n\nFor testing multiple concurrent microscopes and acquisition sessions simultaneously, use the multi-microscope test runner:\n\nScript parameters (all optional):\nNumber of microscopes (default: )\nRecording file path (default: )\nEPU base directory (default: )\n   - Each microscope gets a separate directory: , , etc.\nMax delay in seconds (default: )\n\nEnvironment variable overrides:\n: Override default recording path\n: Override default EPU directory\n\nWorkspace structure: Scripts assume a multi-repo workspace layout where this repo lives at .\n\nWhat the script does:\nCreates timestamped test results directory in \nActivates venv and loads environment variables from \nResets database to clean state\nStarts single API server and consumer (shared by all microscopes)\nStarts N agent instances, each with unique:\n  - Agent ID (e.g., , , )\n  - Session ID (e.g., , , )\n  - EPU directory for playback isolation\nRuns N concurrent playback instances to separate directories\nCollects and verifies:\n  - Filesystem statistics per microscope\n  - Database counts per acquisition\n  - Agent session associations\n  - Data separation between acquisitions\nSaves all logs to test results directory (N agent logs, N playback logs, 1 API log, 1 consumer log)\nCleans up background processes and directories on exit\n\nWhen to use:\nTesting multi-microscope facility scenarios\nValidating data isolation in the database\nTesting SSE routing correctness (each agent receives only its own instructions)\nIdentifying race conditions and resource contention\nTesting concurrent agent/backend operations\nVerifying no data leakage between acquisitions\n\nSuccess criteria:\nEach agent creates a separate acquisition in the database\nAll acquisitions have completely separate data (no shared grids/gridsquares/foilholes)\nEach agent receives SSE instructions only for its own session\nDatabase counts match filesystem counts for each microscope\nNo errors in any service logs\nData separation verification passes\n\nExample output:\n\nOverview\n\nThe test setup simulates a complete SmartEM workflow:\nLocal k3s cluster: Runs PostgreSQL and RabbitMQ services\nHost OS services: Runs SmartEM backend API and worker for easier debugging\nMicroscope simulation: Plays back pre-recorded microscopy sessions\nAgent monitoring: SmartEM agent watches for file changes and processes data\n\nPrerequisites\n\nEnvironment Setup\nPython 3.12+ with venv activated: \nFull development install: \nLocal k3s cluster running: \nEnvironment file:  (created from  - see Environment File Setup below)\n\nTest Data\nPre-recorded microscope sessions are stored in :\n(recommended for testing, 8389 events)\n\nNote: Scripts auto-detect workspace structure. Override with  env var if needed.\n\nDirectory Structure\n\nEnvironment File Setup\n\nThe  file configures services to run on host OS whilst connecting to k3s infrastructure.\n\nSource for initial values: Copy from  and configure credentials\n\nRequired configuration:\nPostgreSQL: Point to k3s NodePort \nRabbitMQ: Point to k3s NodePort \nBackend API: Run on host OS (typically )\nAgent: Run on host OS\n\nExample :\n\nUsage: Automatically loaded by  script\n\nDatabase Operations\n\nCreating Fresh Database\nRun  to drop all tables and recreate schema with indexes:\n\nNote: This script uses  which:\nDrops all existing tables and custom enum types\nCreates all tables from SQLModel",
      "excerpt": "End-to-End Development Test Runs This document describes the complete workflow for running repeatable end-to-end tests of the SmartEM system using pre-recorded microscope sessions. It serves as both a..."
    },
    {
      "id": "doc-34",
      "title": "Generate Documentation",
      "href": "/docs/development/generate-docs",
      "section": "Development",
      "content": "Generate Documentation\n\nDocumentation Generation",
      "excerpt": "Generate Documentation Documentation Generation"
    },
    {
      "id": "doc-35",
      "title": "How to Sync GitHub Labels",
      "href": "/docs/development/github-labels",
      "section": "Development",
      "content": "How to Sync GitHub Labels\n\nThis guide explains how to manage GitHub labels across SmartEM repositories using the label sync tool.\n\nOverview\n\nThe SmartEM ecosystem uses a standardised set of GitHub labels across four repositories:\n\nLabels are defined in  and synced using .\n\nPrerequisites\n\nNode.js 18+\nAuthentication via one of:\n  - GitHub CLI () installed and authenticated ()\n  -  environment variable with repo scope\nFor sync operations: write access to target repositories\n\nLabel Categories\n\nTypes of Work\n\nLabels that categorise the nature of work being done:\n\n Colour \n\n teal \n green \n red \n purple \n orange \n cyan \n slate \n maroon \n brown \n dark cyan \n\nSystem Components\n\nLabels that identify which part of the system is affected:\n\n Colour Family \n\n Blue (dark) \n Blue (mid) \n Blue (light) \n Gold \n Green \n Purple \n Pink (dark) \n Pink (mid) \n Pink (light) \n Pink (lightest) \n\nPer-Repository Configuration\n\nNot all repos need all labels. The config defines which label sets each repo receives:\n\n Labels Applied \n\n Types of work + system components \n Types of work only \n\nCurrent assignments in :\n\n Mode ------------\n   smartem-decisions \n   fandanGO-cryoem-dls \n\nSystem component labels are only relevant in the devtools index repo where cross-repo issues are tracked.\n\nUsage\n\nCheck Label Conformity\n\nTo check if all repositories have the correct labels without making changes:\n\nOr directly:\n\nThis will:\nFetch existing labels from each repository\nCompare against the defined labels in \nReport any discrepancies (missing, extra, or outdated labels)\nExit with code 1 if any repository is non-conforming\n\nSync Labels\n\nTo synchronise labels across all repositories:\n\nOr directly:\n\nThis will:\nDelete labels not in the definition (extra labels)\nCreate labels that are missing\nUpdate labels where description or colour has changed\n\nTarget Specific Repositories\n\nTo sync only specific repositories:\n\nVerbose Output\n\nFor detailed output including conforming labels:\n\nCI/CD Integration\n\nThe  workflow automates label management:\n\nOn push to main: Runs  mode when  or  changes\nManual dispatch: Can trigger  mode via GitHub Actions UI\n\nRunning Sync via CI/CD\n\nGo to Actions tab in smartem-devtools repository\nSelect \"Gitflow\" workflow\nClick \"Run workflow\"\nSelect  mode\nClick \"Run workflow\"\n\nModifying Labels\n\nTo add, remove, or modify labels:\n\nEdit \nRun  to verify changes\nRun  to apply changes\nCommit and push changes\n\nThe CI/CD workflow will verify conformity on push.\n\nAuthentication Backends\n\nThe sync script supports two authentication backends with automatic fallback:\n\nPrimary: GitHub CLI\n\nThe preferred method uses  CLI for shell-based operations:\n\nFallback: GitHub REST API\n\nIf  CLI is unavailable or not authenticated, the script falls back to the GitHub REST API using a personal access token:\n\nGenerate a token at https://github.com/settings/tokens with  scope.\n\nThe script automatically selects the best available backend and displays which one is in use:\n\nTroubleshooting\n\nAuthentication Errors\n\nEnsure  CLI is authenticated:\n\nOr set  environment variable as fallback.\n\nPermission Denied\n\nFor sync operations, you need write access to all target repositories. The CI/CD workflow uses a PAT stored as  secret.\n\nLabel Already Exists\n\nIf a label creation fails because it already exists, the script will continue. Run with  to see details.",
      "excerpt": "How to Sync GitHub Labels This guide explains how to manage GitHub labels across SmartEM repositories using the label sync tool. Overview The SmartEM ecosystem uses a standardised set of GitHub labels..."
    },
    {
      "id": "doc-36",
      "title": "Development",
      "href": "/docs/development",
      "section": "Development",
      "content": "Development\n\nDocumentation for contributing to the SmartEM codebase.\n\nTopics\n\nContributing\n\nContributing Guide - Code style, pull requests, reviews\n\nTools and Testing\n\nDevelopment Tools - Utility tools for development, testing, and maintenance\nE2E Simulation - End-to-end development simulation setup\nMCP Interface - SmartEM MCP server for Claude Code integration\n\nDocumentation and CI/CD\n\nGenerate Documentation - Building the documentation locally\nGitHub Labels - Synchronising GitHub labels across repositories",
      "excerpt": "Development Documentation for contributing to the SmartEM codebase. Topics Contributing Contributing Guide - Code style, pull requests, reviews Tools and Testing Development Tools - Utility tools for..."
    },
    {
      "id": "doc-37",
      "title": "Using SmartEM MCP Interface",
      "href": "/docs/development/mcp-interface",
      "section": "Development",
      "content": "Using SmartEM MCP Interface\n\nThe SmartEM MCP (Model Context Protocol) interface provides natural language querying capabilities for microscopy session data. Built with FastMCP 2.0, it supports both filesystem-based parsing and API-based queries for comprehensive data access.\n\nOverview\n\nThe MCP interface consists of:\nFastMCP Server: Provides natural language query capabilities via MCP protocol using FastMCP framework\nMCP Client: Python client for programmatic access with simplified FastMCP patterns\nCLI Interface: Command-line tools for interactive usage\n\nInstallation\n\nInstall with MCP dependencies (includes FastMCP 2.0 framework):\n\nThis installs the required FastMCP 2.0 framework and SmartEM MCP components.\n\nConfiguration\n\nThe MCP server can be configured using environment variables. A template configuration file is provided:\n\nAvailable configuration options (see  for details):\n: Path to EPU sessions directory (for filesystem adapter)\n: SmartEM backend API URL (for API adapter)\n: Data adapter selection ( or )\n: Logging level (, , , )\n\nQuick Start\n\nInteractive Query Mode\n\nStart interactive mode for natural language questions:\n\nExample questions:\n\"Show me a summary of session /path/to/epu/directory\"\n\"Find low quality items in /path/to/epu with threshold 0.3\"\n\"What are the recent acquisitions?\"\n\nCommand Line Usage\n\nParse EPU directory:\n\nFind low quality items:\n\nQuery recent acquisitions (requires API):\n\nProgrammatic Usage\n\nData Sources\n\nFilesystem Queries\n\nDirect parsing of EPU XML files using existing  tools:\n\nUse case: Ad-hoc analysis, debugging, development\nCapabilities: Full EPU directory parsing, quality analysis\nRequirements: Direct filesystem access to EPU data\n\nAPI Queries\n\nQuery historical and in-flight sessions via SmartEM API:\n\nUse case: Historical analysis, live session monitoring\nCapabilities: Acquisition status, grid processing, real-time data\nRequirements: Running SmartEM backend service\n\nClaude Code Integration\n\nThe SmartEM MCP server integrates seamlessly with Claude Code, allowing natural language queries about microscopy data directly within your development environment.\n\nPrerequisites\n\nEnsure MCP dependencies are installed (includes FastMCP 2.0):\n\nVerify the installation:\n\nRegistration with Claude Code\n\nOption 1: Using Claude CLI (Recommended)\n\nRegister the SmartEM MCP server using the Claude CLI:\n\nOption 2: Manual Configuration\n\nIf you prefer manual configuration, add the server to your Claude Code settings:\n\nFor user-wide registration:\nEdit :\n\nFor project-scoped registration:\nCreate  in your project root:\n\nRegistration Options\n\nEnvironment Variables\n\nConfigure the MCP server behaviour through environment variables:\n\n: Base URL for SmartEM API (default: )\n: Logging level (, , , )\n: Python executable path if not in system PATH\n\nExample with custom environment:\n\nScope Options\n\nUser scope (default): Available across all Claude Code projects\nProject scope: Only available within the current project directory\n\nVerification\n\nCheck Registration Status\n\nTest Connection\n\nStart a new Claude Code session and verify the SmartEM tools are available:\n\nYou should see the following tools available:\n- Parse EPU microscopy directories\n- Find low-quality images and foil holes\n- Query recent acquisition sessions\n- Get grid processing status\n\nNote: The client provides high-level methods such as  which internally call the corresponding MCP tools.\n\nTest Basic Functionality\n\nTry a simple query in Claude Code:\n\n\"Parse the EPU directory at /path/to/your/epu/session\"\n\nUsage in Claude Code\n\nOnce registered, you can interact with SmartEM data using natural language within Claude Code:\n\nDirectory Analysis\n\n\"Show me a comprehensive analysis of the EPU session at /data/microscopy/session001\"\n\n\"What grids and grid squares are available in /path/to/epu/directory?\"\n\nQuality Assessment\n\n\"Find all images with quality scores below 0.4 in the EPU directory /data/session001\"\n\n\"Show me foil holes that need attention from /path/to/epu with quality threshold 0.3\"\n\nSession Monitoring\n\n\"What are the 5 most recent microscopy acquisitions?\"\n\n\"Check the processing status of grid uuid-12345\"\n\nAdvanced Queries\n\n\"Compare quality metrics between /data/session001 and /data/session002\"\n\n\"Generate a report for all low-quality items found today\"\n\nServer Mode (Standalone)\n\nFor advanced users or debugging, run the MCP server in standalone mode:\n\nThis starts the server with stdio communication, primarily useful for:\nDebugging MCP protocol issues\nCustom client integrations\nDevelopment and testing\n\nAvailable Tools\n\nParse EPU microscopy directory and extract comprehensive session data.\n\nParameters:\n(required): Path to EPU output directory containing EpuSession.dm\n\nReturns: Acquisition data, grids, grid squares, and statistics\n\nFind foil holes and micrographs with quality scores below threshold.\n\nParameters:\n(required): Path to EPU directory\n(optional): Quality threshold (default: 0.5)\n(optional): \"filesystem\" or \"api\" (default: filesystem)\n\nReturns: Li",
      "excerpt": "Using SmartEM MCP Interface The SmartEM MCP (Model Context Protocol) interface provides natural language querying capabilities for microscopy session data. Built with FastMCP 2.0, it supports both fil..."
    },
    {
      "id": "doc-38",
      "title": "Development Tools",
      "href": "/docs/development/tools",
      "section": "Development",
      "content": "Development Tools\n\nCollection of utility tools for development, testing, and maintenance of the SmartEM Decisions project.\n\nXML Formatting Tools\n\nFormat XML Files for Human Readability\n\nTransform single-line XML and .dm files into human-readable format with proper indentation:\n\nData Analysis and Debugging Tools\n\nFind Foil Hole Manifest Duplicates\n\nIdentify duplicate foil hole manifests within directory structures to detect data inconsistencies:\n\nFile Size Analysis\n\nList files matching specific patterns, sorted by size for storage analysis:\n\nTest Dataset Management\n\nFile Extension Analysis\n\nAnalyse the composition of test datasets by file type:\n\nDataset Size Reduction\n\nReduce test dataset storage requirements whilst maintaining directory structure:\n\nWarning: This command permanently removes file contents. Use only on test datasets, not production data.\n\nDevelopment Monitoring\n\nDirectory Growth Monitoring\n\nMonitor directory metrics during data acquisition or processing:\n\nThis tool is particularly useful for monitoring EPU data acquisition progress or debugging processing pipeline performance.\n\nMessage Testing and Communication Tools\n\nExternal Message Simulator\n\nComprehensive CLI tool for simulating external data processing messages that would normally come from ML pipelines and image processing systems:\n\nAvailable Message Types:\n- Motion correction processing finished\n- CTF estimation completed\n- Particle identification finished\n- Particle quality assessment done\n- ML prediction for grid square quality\n- ML prediction for foilhole targeting\n- ML model parameter updates\n\nSSE Client Testing\n\nExample client for testing agent-backend communication via Server-Sent Events:\n\nThis tool is particularly useful for:\nTesting the complete agent-backend communication pipeline\nValidating instruction delivery and acknowledgement mechanisms\nPerformance testing of SSE communication\nDevelopment of new agent integrations\n\nBoth tools work together to simulate the complete external data flow into the SmartEM system, enabling comprehensive testing without requiring actual microscopy equipment or external processing systems.\n\nAdditional Development Commands\n\nPre-commit Workflow\n\nMaintain code quality during development:\n\nTesting and Quality Assurance",
      "excerpt": "Development Tools Collection of utility tools for development, testing, and maintenance of the SmartEM Decisions project. XML Formatting Tools Format XML Files for Human Readability Transform single-l..."
    },
    {
      "id": "doc-39",
      "title": "Installation",
      "href": "/docs/getting-started/for-developers",
      "section": "Getting Started",
      "content": "Installation\n\nCheck Your Version of Python\n\nSmartEM Decisions requires Python 3.12 or later for optimal performance and access to modern typing features. You can check your current Python version by entering the following command in a terminal:\n\nNote: The project specifically requires Python 3.12+ to utilise advanced typing features and maintain compatibility with the latest scientific computing libraries.\n\nCreate a Virtual Environment\n\nIt is strongly recommended that you install SmartEM Decisions within a virtual environment to prevent conflicts with existing Python installations and maintain a clean development environment:\n\nFor development work, consider creating the virtual environment within the project directory:\n\nInstalling the Library\n\nStandard Installation\n\nYou can install the library and its core dependencies using :\n\nDevelopment Installation\n\nFor development work or if you require the latest features, install directly from the source repository:\n\nInstallation with Optional Dependencies\n\nInstall with specific feature sets:\n\nVerify Installation\n\nVerify that all components are correctly installed by running:\n\nYou can also verify the CLI tools are available:\n\nNext Steps\n\nOnce installation is complete, you can proceed to:\n\nRun the backend service\nDeploy using containers\nSet up the development environment\nConfigure logging",
      "excerpt": "Installation Check Your Version of Python SmartEM Decisions requires Python 3.12 or later for optimal performance and access to modern typing features. You can check your current Python version by ent..."
    },
    {
      "id": "doc-40",
      "title": "Getting Started",
      "href": "/docs/getting-started",
      "section": "Getting Started",
      "content": "Getting Started\n\nThis section provides entry points for different audiences working with SmartEM.\n\nBy Audience\n\nFor Developers\n\nSet up a local development environment to contribute to SmartEM.\n\nDevelopment Environment Setup - Install dependencies, configure tools, run services locally\n\nFor Operators\n\nDeploy and operate SmartEM in production environments.\n\nSee Operations for Kubernetes deployment, container configuration, and environment setup\n\nFor Integrators\n\nUse SmartEM APIs to build integrations.\n\nSee Backend API Documentation for interactive API documentation\nSee HTTP API Client for programmatic access",
      "excerpt": "Getting Started This section provides entry points for different audiences working with SmartEM. By Audience For Developers Set up a local development environment to contribute to SmartEM. Development..."
    },
    {
      "id": "doc-41",
      "title": "Glossary",
      "href": "/docs/glossary",
      "section": "Documentation",
      "content": "Glossary\n\nEPU\n\nWindows desktop application developed by ThermoFisher, serving as the original piece of electron microscopy\nacquisition software. EPU (Enhanced Productivity User interface) provides comprehensive control over data collection\nparameters and automation workflows for cryo-electron microscopy experiments.\n\nSPA\n\nSingle Particle Analysis - A computational method in cryo-electron microscopy used to determine the\nthree-dimensional structure of biological macromolecules from individual particle images embedded in vitreous ice.\n\nAthena / Athena API\n\nDecision-making software developed by ThermoFisher for automated cryo-electron microscopy workflows. The Athena API\nprovides programmatic access to session management, decision recording, and algorithm results for optimised data\ncollection strategies.\n\nARIA\n\nAutomated Real-time Image Analysis system developed by INSTRUCT-ERIC for real-time processing and quality assessment\nof cryo-electron microscopy data during acquisition. More information available at:\nhttps://instruct-eric.org/help/about-aria\n\nAtlas\n\nA comprehensive overview image of the entire grid, composed of individual tiles arranged in a 5x5 grid pattern. The\natlas provides spatial context and navigation reference for subsequent high-magnification data collection across the\nspecimen grid.\n\nGrid Square\n\nIndividual sections of the electron microscopy grid that contain the specimen of interest. Grid squares are selected\nbased on ice quality, specimen distribution, and other quality metrics for detailed imaging at higher magnifications.\n\nFoil Hole\n\nCircular openings in the carbon support film of the electron microscopy grid where the biological specimen is\nsuspended in vitreous ice. These holes provide areas of minimal background interference for high-resolution imaging of\nembedded particles.\n\nMicrograph\n\nHigh-resolution electron microscopy images captured from individual foil holes or grid squares, containing the\nembedded biological specimens used for subsequent image processing and structural analysis.",
      "excerpt": "Glossary EPU Windows desktop application developed by ThermoFisher, serving as the original piece of electron microscopy acquisition software. EPU (Enhanced Productivity User interface) provides compr..."
    },
    {
      "id": "doc-42",
      "title": "Container User Configuration",
      "href": "/docs/operations/container-user-configuration",
      "section": "Operations",
      "content": "Container User Configuration\n\nThis guide explains how to configure the SmartEM Decisions container to run as a non-root user with specific UID/GID, which is essential for certain deployment scenarios at Diamond Light Source (DLS).\n\nOverview\n\nThe SmartEM Decisions Dockerfile supports two distinct operational modes:\n\nDefault Mode (Root User): For CI/CD pipelines and local development\nCustom User Mode: For production deployments requiring filesystem access with specific permissions\n\nWhy Non-Root Users?\n\nAt Diamond Light Source, the HTTP API service needs to access microscopy images stored on the  filesystem. This filesystem:\n\nContains electron microscopy data from EPU sessions\nCannot be mounted in containers with root privileges due to security policies\nRequires the container to run with a specific UID/GID that has read permissions\nIs accessed by image serving endpoints in the HTTP API\n\nWithout proper user configuration, the container cannot access  and image serving endpoints will fail.\n\nBuild Arguments\n\nThe Dockerfile accepts three build arguments that control user/group creation:\n\n Default \n\nDefault Behavior: Root User\n\nWhen build arguments are not specified (or explicitly set to defaults), the container runs as root:\n\nUse cases for root mode:\nCI/CD pipelines (GitHub Actions, GitLab CI)\nLocal development environments\nEnvironments without specific filesystem permission requirements\nTesting and debugging\n\nImplications:\nContainer has full privileges\nNo user creation step is executed\nAll files owned by root (UID 0, GID 0)\nCannot mount DLS filesystem in production\n\nCustom User Mode: DLS Deployment\n\nFor production deployment at DLS, build the container with specific UID/GID:\n\nWhat happens during build:\nA group is created with the specified  and \nA user is created with the specified , belonging to the group\nAll application files (, , ) are set to be owned by this user\nThe container will execute processes as this user (not root)\n\nUse cases for custom user mode:\nDLS production/staging deployments\nAny environment requiring specific filesystem permissions\nSecurity-hardened deployments following least-privilege principles\n\nImplications:\nContainer runs with limited privileges\nCan access  filesystem when mounted with matching permissions\nImage serving endpoints function correctly\nMore secure than running as root\n\nVolume Mounting Considerations\n\nThe /dls Directory\n\nThe  directory is not created in the Docker image. It should be mounted at runtime:\n\nIn Kubernetes, this is typically done via a PersistentVolume or hostPath mount:\n\nWhen /dls is Not Mounted\n\nIf the  directory is not mounted or does not exist:\n\nThe container will start normally\nMost API endpoints will function correctly\nImage serving endpoints will return 404 errors when image paths reference \nError messages will indicate that the file cannot be found\n\nThis is by design - the container is operational without , but image serving functionality is unavailable.\n\nOptional Mount Strategy\n\nThe  mount should be considered optional for development but required for production:\n\nDevelopment/Testing: Run without  mount for testing non-image features\nStaging: Mount a subset of  or test data directory\nProduction: Mount full  filesystem with proper permissions\n\nImage Serving Endpoints\n\nThe HTTP API provides endpoints for serving microscopy images:\n\n- Serve grid atlas images\n- Serve grid square images\n\nThese endpoints:\nQuery the database for image file paths\nRead image files from the filesystem (typically )\nProcess and return images in PNG format\n\nRequirements:\nContainer must run as user with read access to image files\nImage paths in database must be accessible by the container user\nFilesystem must be mounted at the correct path\n\nError handling:\nIf image path is not in database: Returns 404 \"Grid square image unknown\"\nIf file doesn't exist: Returns 404 or file system error\nIf permissions denied: Returns 500 error with permission details\n\nSecurity Considerations\n\nPrinciple of Least Privilege\n\nRunning containers as non-root is a security best practice:\n\nRisk Reduction: Limited damage if container is compromised\nPolicy Compliance: Many organizations require non-root containers\nAudit Trail: Clear user identity in logs and process lists\n\nChoosing UID/GID\n\nWhen selecting UID/GID for custom user mode:\n\nMatch Filesystem Permissions: Use UID/GID that has read access to required files\nAvoid System IDs: Don't use UIDs below 1000 (reserved for system users)\nDocument Values: Keep a record of which UID/GID is used in each environment\nConsistency: Use the same UID/GID across all pods in the same environment\n\nFile Ownership\n\nAll files in the container are owned by the specified user:\n\nThis ensures the application can read its own files while running as non-root.\n\nBuilding for Different Environments\n\nLocal Development\n\nCI/CD Pipeline\n\nStaging Environment\n\nProduction Environment\n\nKubernetes Deployment Configuration\n\nExample: HTTP API with Custom User\n\nIf you've built the image with custom UID/G",
      "excerpt": "Container User Configuration This guide explains how to configure the SmartEM Decisions container to run as a non-root user with specific UID/GID, which is essential for certain deployment scenarios a..."
    },
    {
      "id": "doc-43",
      "title": "Containerization",
      "href": "/docs/operations/containerization",
      "section": "Operations",
      "content": "Containerization\n\nThis guide covers building and managing SmartEM Decisions container images.\n\nDocker/Podman Operations\n\nBasic Build (Default Configuration)\n\nBuild the container with default settings (runs as root):\n\nBuild with Custom User (DLS Deployment)\n\nFor production deployment at Diamond Light Source, build with custom UID/GID to enable  filesystem access:\n\nSee Container User Configuration for detailed information about build arguments and deployment scenarios.\n\nRunning Containers\n\nCleanup\n\nTagging and Pushing Images\n\nGitHub Container Registry (Current)\n\nTagging Strategies\n\nMulti-Stage Build Architecture\n\nThe Dockerfile uses a multi-stage build process:\n\ndeveloper stage: Base Python image with system dependencies\nbuild stage: Installs Python packages and application code\nruntime stage: Slim image with only runtime dependencies and built application\n\nThis approach:\nMinimizes final image size\nSeparates build tools from runtime environment\nEnables efficient layer caching\n\nBuild Arguments Reference\n\n Default \n\nRelated Documentation\n\nContainer User Configuration - Detailed guide on UID/GID configuration\nRun in a Container - Using pre-built containers\nKubernetes Deployment - Deploying to Kubernetes\n\nReferences\n\nGitHub Container Registry\nDocker Multi-stage Builds\nPodman Documentation",
      "excerpt": "Containerization This guide covers building and managing SmartEM Decisions container images. Docker/Podman Operations Basic Build (Default Configuration) Build the container with default settings (run..."
    },
    {
      "id": "doc-44",
      "title": "Configure Environment Variables",
      "href": "/docs/operations/environment-variables",
      "section": "Operations",
      "content": "Configure Environment Variables\n\nThis guide explains the different environment configuration files used in the SmartEM Decisions project and their specific purposes.\n\nOverview\n\nThe project uses multiple environment configuration patterns for different deployment scenarios:\n\n Purpose  Created From --------------------------\n Template for local development  N/A    Yes \n Template for K8s development  N/A    Yes \n Template for K8s staging  N/A    Yes \n Template for K8s production  N/A    Yes \n Template for MCP configuration  N/A    Yes \n\nEnvironment File Types\n\nLocal Development: \n\nUse Case: Running backend services directly on your host OS (outside containers) while connecting to K8s infrastructure.\n\nWhen to Use:\nDay-to-day development with \nRunning API server locally with \nE2E testing with \nManual debugging and development workflows\n\nSetup:\n\nKey Configuration:\n\nUsed By:\nPython code via  in , \nDocker  when running containers outside Kubernetes\nE2E test scripts ()\n\nKubernetes Development: \n\nUse Case: Deploying SmartEM to local K8s cluster (k3s) for development.\n\nWhen to Use:\nRunning \nSetting up local development cluster with all services\n\nSetup:\n\nKey Configuration:\n\nUsed By:\nscript to create K8s Secrets and ConfigMaps\n\nKubernetes Staging: \n\nUse Case: Deploying to staging Kubernetes cluster.\n\nSetup:\n\nKey Differences from Development:\nReal credentials (not test values)\nProduction-like service names and ports\nStaging-specific CORS origins\nMay use sealed secrets instead of plain credentials\n\nUsed By:\nCI/CD pipelines for staging deployments\n(if adapted for remote clusters)\n\nKubernetes Production: \n\nUse Case: Deploying to production Kubernetes cluster.\n\nSetup:\n\nSecurity Considerations:\nShould use Sealed Secrets (encrypted) instead of plain  files\nCredentials should be rotated regularly\nAccess to this file should be strictly controlled\nSee Manage Kubernetes Secrets for best practices\n\nUsed By:\nProduction CI/CD pipelines\nProduction deployment automation\n\nMCP Configuration: \n\nUse Case: Configuring Model Context Protocol (MCP) server for Claude Code integration.\n\nSetup:\n\nKey Configuration:\n\nUsed By:\nMCP server when run via Claude Code\nSee Use MCP Interface for details\n\nQuick Start: First-Time Setup\n\nFor local development on a new machine:\n\nHow Environment Variables Are Loaded\n\nPython Services\n\nAll Python services use :\n\nLoading Priority (first wins):\nAlready exported environment variables (from  or )\nVariables in  file (loaded by )\nHardcoded defaults in code (if any)\n\nImportant for Testing:\nWhen running E2E tests or manual services, you must export variables BEFORE starting services:\n\nDocker Containers\n\nContainer behaviour depends on runtime environment:\n\nKubernetes Deployments\n\nWhen running in Kubernetes:\nConfigMaps provide non-sensitive configuration (hosts, ports, URLs)\nSecrets provide sensitive data (passwords, tokens)\nfiles are NOT used inside containers\nAll configuration comes from K8s resources created by \n\nCommon Scenarios\n\nScenario 1: Local Development with K8s Infrastructure\n\nGoal: Run backend/agent on host OS, connect to K8s database/RabbitMQ.\n\nScenario 2: Full K8s Deployment\n\nGoal: Everything runs in K8s (no host OS services).\n\nScenario 3: E2E Testing\n\nGoal: Run automated E2E tests with recorded microscope data.\n\nTroubleshooting\n\nIssue: Services can't connect to database\n\nSymptom: \n\nCheck:\nIs K8s cluster running? \nAre NodePorts accessible?  (should refuse connection but not timeout)\nIs  configured correctly?\n\nIssue: Docker password invalid in dev-k8s.sh\n\nSymptom: \n\nSolution:\n\nIssue: Services using wrong database\n\nSymptom: Services connect to different database than expected.\n\nCheck loading priority:\n\nRelated Documentation\n\nRun Backend Services - Starting backend API and consumer\nRun E2E Development Simulation - E2E testing workflows\nDeploy to Kubernetes - K8s deployment guide\nManage Kubernetes Secrets - Sealed Secrets and security\nDatabase Migrations - Alembic migration workflow\nUse MCP Interface - Claude Code MCP integration",
      "excerpt": "Configure Environment Variables This guide explains the different environment configuration files used in the SmartEM Decisions project and their specific purposes. Overview The project uses multiple..."
    },
    {
      "id": "doc-45",
      "title": "Operations",
      "href": "/docs/operations",
      "section": "Operations",
      "content": "Operations\n\nCross-cutting operational documentation for deploying and running SmartEM in production.\n\nTopics\n\nKubernetes\n\nKubernetes Deployment - Deploying SmartEM to Kubernetes clusters\nKubernetes Secrets - Managing secrets and sensitive configuration\n\nContainers\n\nContainerisation - Building container images\nRunning Containers - Container execution and debugging\nContainer User Configuration - User and permission configuration\n\nConfiguration\n\nEnvironment Variables - Complete configuration reference\nLogging - Log configuration and structured logging",
      "excerpt": "Operations Cross-cutting operational documentation for deploying and running SmartEM in production. Topics Kubernetes Kubernetes Deployment - Deploying SmartEM to Kubernetes clusters Kubernetes Secret..."
    },
    {
      "id": "doc-46",
      "title": "Managing Kubernetes Secrets",
      "href": "/docs/operations/kubernetes-secrets",
      "section": "Operations",
      "content": "Managing Kubernetes Secrets\n\nThis guide explains how to securely manage secrets in the SmartEM Decisions project using Bitnami Sealed Secrets.\n\nOverview\n\nThe SmartEM Decisions project uses Bitnami Sealed Secrets to securely manage sensitive\nconfiguration data such as database credentials and message queue passwords. Sealed Secrets provide a secure alternative to\nstoring plain-text secrets in version control.\n\nWhy Sealed Secrets?\n\nSealed Secrets offer several security advantages over traditional Kubernetes secrets:\n\nVersion Control Safe: Sealed secrets are encrypted and safe to commit to Git repositories\nAsymmetric Encryption: Uses public/private key cryptography for maximum security\nCluster-Specific: Secrets are encrypted for a specific cluster and cannot be used elsewhere\nAutomatic Decryption: The sealed-secrets controller automatically decrypts secrets in the cluster\nAudit Trail: All secret changes are tracked in version control with proper attribution\n\nHow It Works\n\nPublic Key Encryption: Secrets are encrypted using the cluster's public key\nSafe Storage: Encrypted sealed secrets are committed to version control\nAutomatic Decryption: The sealed-secrets controller watches for sealed secrets and creates regular Kubernetes secrets\nApplication Access: Applications access secrets normally via environment variables or mounted volumes\n\nPrerequisites\n\nBefore managing sealed secrets, ensure you have the required tools installed:\n\nRequired Tools\n\nkubectl: Kubernetes command-line tool with cluster access\nkubeseal: Bitnami Sealed Secrets CLI tool\nopenssl: For secure password generation (development environments)\n\nInstalling kubeseal\n\nCluster Requirements\n\nThe sealed-secrets controller must be installed in your Kubernetes cluster. For Diamond Light Source clusters, this is\ntypically pre-installed. Verify the controller is running:\n\nQuick Start\n\nThe project includes a convenient script that handles the entire sealed secret generation process:\n\nEnvironment-Specific Workflows\n\nDevelopment Environment\n\nDevelopment environments use automatically generated secure passwords for convenience:\n\nThis will:\nGenerate cryptographically secure random passwords\nCreate sealed secrets for the  namespace\nDisplay a summary of generated usernames (passwords remain sealed)\nUpdate \n\nStaging and Production Environments\n\nProduction and staging environments require interactive credential input for security:\n\nYou'll be prompted to provide:\nPostgreSQL username and password\nRabbitMQ username and password\n\nSecurity Note: Passwords are entered without echo (not displayed on screen) and never appear in shell history.\n\nManual Secret Management\n\nFor advanced use cases, you can manually create sealed secrets:\n\nCreate Temporary Secret\n\nGenerate Sealed Secret\n\nApply Sealed Secret\n\nSecret Rotation\n\nRegular secret rotation is essential for security. Follow these steps to rotate secrets:\n\nGenerate New Credentials\n\nApply Updated Secrets\n\nRestart Applications\n\nRestart application pods to pick up new secrets:\n\nVerify Application Health\n\nIntegration with Kustomize\n\nSealed secrets integrate seamlessly with the project's Kustomize structure:\n\nDirectory Structure\n\nApplying Changes\n\nTroubleshooting\n\nCommon Issues\n\nkubeseal command not found\n\ncannot fetch certificate error\n\nsecret not being decrypted\n\nSealed Secret Validation\n\nVerify sealed secrets are correctly formatted:\n\nApplication Connection Issues\n\nIf applications cannot connect after secret rotation:\n\nScript Debugging\n\nFor issues with the generation script:\n\nSecurity Best Practices\n\nSecret Generation\n\nUse Strong Passwords: Generate passwords with sufficient entropy (minimum 24 characters)\nUnique Credentials: Use different credentials for each environment\nRegular Rotation: Rotate secrets quarterly or after security incidents\nPrinciple of Least Privilege: Grant minimal required database/queue permissions\n\nAccess Control\n\nNamespace Isolation: Deploy sealed secrets to appropriate namespaces\nRBAC Controls: Restrict access to sealed secret resources\nAudit Logging: Monitor sealed secret creation and modification\nBackup Strategy: Maintain secure backups of unsealed credentials\n\nDevelopment Workflow\n\nNever Commit Plain Secrets: Always use sealed secrets in version control\nValidate Before Commit: Verify sealed secrets are properly encrypted\nEnvironment Separation: Keep development and production secrets separate\nCode Review: Review all secret-related changes before merging\n\nAdvanced Usage\n\nCustom Secret Names\n\nTo use different secret names:\n\nMultiple Secret Sources\n\nFor complex applications requiring multiple secret sources:\n\nCross-Cluster Migration\n\nWhen moving between clusters, sealed secrets must be regenerated:\n\nFiles and Structure\n\nFurther Reading\n\nBitnami Sealed Secrets Documentation\nKubernetes Secrets Documentation\nDiamond Light Source Kubernetes Standards\nSecurity Best Practices for Kubernetes",
      "excerpt": "Managing Kubernetes Secrets This guide explains how to securely manage secrets in the SmartEM Decisions project using Bitnami Sealed Secrets. Overview The SmartEM Decisions project uses Bitnami Sealed..."
    },
    {
      "id": "doc-47",
      "title": "Kubernetes Deployment",
      "href": "/docs/operations/kubernetes",
      "section": "Operations",
      "content": "Kubernetes Deployment\n\nThis directory contains Kubernetes deployment configurations for SmartEM Backend across different environments.\n\nQuick Start (Development)\n\nFor local development, use the convenient script that provides a docker-compose-like experience:\n\nAccess URLs\nOnce the environment is running, you can access:\nAdminer (Database UI): http://localhost:30808\nRabbitMQ Management: http://localhost:30673\nSmartEM Backend HTTP API: http://localhost:30080/health\nAPI Documentation: http://localhost:30080/docs\n\nNote: The script automatically handles GitHub Container Registry authentication and waits for all pods to be ready.\n\nKubernetes Structure\n\nSecurity: Sealed Secrets\n\nThe project uses Bitnami Sealed Secrets for secure credential management. Before\ndeploying to any environment, you must generate the appropriate sealed secrets:\n\nGenerate Secrets for Development\n\nGenerate Secrets for Production\n\nSealed secrets are encrypted with the cluster's public key and safe to commit to version control. The sealed-secrets\ncontroller automatically decrypts them into regular Kubernetes secrets that applications can use.\n\nSecurity Note: Never commit plain-text secrets to version control. Always use sealed secrets for credential management.\n\nFor comprehensive secret management documentation, see Managing Kubernetes Secrets.\n\nDetailed Documentation\n\nFor detailed Kubernetes deployment instructions, environment configurations, and troubleshooting, see the k8s directory documentation.",
      "excerpt": "Kubernetes Deployment This directory contains Kubernetes deployment configurations for SmartEM Backend across different environments. Quick Start (Development) For local development, use the convenien..."
    },
    {
      "id": "doc-48",
      "title": "Configure Logging and Verbosity",
      "href": "/docs/operations/logging",
      "section": "Operations",
      "content": "Configure Logging and Verbosity\n\nAll SmartEM services support configurable logging levels to help with debugging and reduce noise in production environments.\n\nCommand Line Verbosity\n\nUse the  and  flags to control verbosity across all SmartEM components:\n\nBackend Services\n\nSmartEM Agent CLI\nAll agent commands support consistent verbosity flags:\n\nAgent-Specific Logging Features\n\nThe watch command provides additional logging controls:\n\nEnvironment Variable Control\n\nFor the HTTP API, you can also control logging via environment variables:\n\nLog Levels\n\nERROR (default): Only critical errors are shown\nINFO (): Informational messages, warnings, and errors\nDEBUG (): All messages including detailed debugging information\n\nLog Content by Component\n\nBackend Services (ERROR level)\nDatabase connection errors\nAPI startup failures\nCritical system errors\n\nBackend Services (INFO level)\nService startup and shutdown\nAPI request summaries\nDatabase connection status\nConfiguration information\n\nBackend Services (DEBUG level)\nIndividual API request details\nDatabase query execution\nMessage queue operations\nDetailed error stack traces\n\nSmartEM Agent (ERROR level)\nFile access permission errors\nAPI connection failures\nCritical parsing errors\n\nSmartEM Agent (INFO level)\nFile detection and processing\nGrid square and foil hole creation\nAPI communication status\nSSE connection events\nHeartbeat transmission status\n\nSmartEM Agent (DEBUG level)\nIndividual file parsing details\nFilesystem event monitoring\nDetailed API request/response data\nSSE message content\nHeartbeat timing information\nConnection retry attempts\n\nProduction Logging Recommendations\n\nDevelopment Environment\n\nTesting Environment\n\nProduction Environment\n\nLog File Management\n\nStructured Logging\nThe agent's  parameter creates structured JSON logs suitable for analysis:\n\nLog Rotation and Management\n\nThis verbosity control system helps reduce log noise during normal operation while providing detailed output when troubleshooting issues. The structured logging format facilitates automated monitoring and analysis of system behaviour.",
      "excerpt": "Configure Logging and Verbosity All SmartEM services support configurable logging levels to help with debugging and reduce noise in production environments. Command Line Verbosity Use the and flags to..."
    },
    {
      "id": "doc-49",
      "title": "PyPI Token Setup for CI/CD",
      "href": "/docs/operations/publish-smartem-workspace-to-pypi",
      "section": "Operations",
      "content": "PyPI Token Setup for CI/CD\n\nThis guide explains how to set up PyPI and TestPyPI accounts and tokens for automated package publishing via GitHub Actions.\n\nTable of Contents\n\nOverview\nPrerequisites\nStep 1: Create PyPI Accounts\nStep 2: Generate API Tokens\nStep 3: Configure GitHub Secrets\nStep 4: First Manual Publish\nStep 5: Update Token Scope\nToken Security Best Practices\nTroubleshooting\nCI/CD Workflow Reference\nMonitoring & Maintenance\nEmergency Procedures\n\nOverview\n\n uses GitHub Actions to automatically publish packages to PyPI and TestPyPI. This requires:\n\nPyPI account - For production releases\nTestPyPI account - For testing releases\nAPI tokens - For authentication from GitHub Actions\nGitHub Secrets - Secure storage of tokens\n\nThis guide walks through the complete setup process.\n\nPrerequisites\n\nBefore starting, ensure you have:\n\nGitHub organisation admin access - To add repository secrets\nTeam email access - For PyPI account registration (e.g., smartem@diamond.ac.uk)\n2FA app - PyPI requires two-factor authentication (e.g., Google Authenticator, Authy)\n\nStep 1: Create PyPI Accounts\n\nProduction PyPI Account\n\nVisit https://pypi.org/account/register/\n\nFill in registration form:\n   - Username: Choose organisational username (e.g.,  or )\n   - Email: Use team email (smartem@diamond.ac.uk)\n   - Password: Strong password (store in team password manager)\n\nVerify email address:\n   - Check team inbox for verification email\n   - Click verification link\n\nEnable 2FA (Required for Publishing):\n   - Go to Account Settings → Two-factor authentication\n   - Scan QR code with authenticator app\n   - Enter 6-digit code to verify\n   - Save recovery codes in secure location (password manager)\n\nTestPyPI Account (Staging Environment)\n\nVisit https://test.pypi.org/account/register/\n\nRepeat registration process:\n   - Use same email as production account\n   - Use different password (or same via password manager)\n   - Important: TestPyPI is a separate system - account doesn't sync with production\n\nVerify email (separate verification email)\n\nEnable 2FA (same process as production)\n\nWhy TestPyPI?\nTest package publishing before production\nVerify metadata, dependencies, installation\nSafe environment for CI/CD testing\n\nStep 2: Generate API Tokens\n\nProduction PyPI Token\n\nLog in to PyPI: https://pypi.org\n\nNavigate to API tokens:\n   - Click username (top right) → Account Settings\n   - Scroll to \"API tokens\" section\n   - Click \"Add API token\"\n\nCreate token:\n   - Token name: \n   - Scope: \n     - For first publish: Select \"Entire account (all projects)\"\n     - After first publish: Select \"Project: smartem-workspace\" (more secure - see Step 5)\n\nCopy token immediately:\n\n   - Warning: Token shown only once. If you lose it, generate a new one.\n   - Store securely: Paste into password manager or directly into GitHub Secrets\n\nTestPyPI Token\n\nLog in to TestPyPI: https://test.pypi.org\n\nNavigate to API tokens: Same process as production\n\nCreate token:\n   - Token name: \n   - Scope: \"Entire account (all projects)\" (TestPyPI can stay account-scoped)\n\nCopy token:\n\n   - Store separately from production token\n\nSecurity Note: Never commit tokens to git or share via email/chat.\n\nStep 3: Configure GitHub Secrets\n\nAdd Secrets to Repository\n\nNavigate to repository secrets:\n   - Go to https://github.com/DiamondLightSource/smartem-devtools/settings/secrets/actions\n   - Requires admin access\n\nAdd production token:\n   - Click \"New repository secret\"\n   - Name:  (exact name - workflow expects this)\n   - Secret: Paste production token ()\n   - Click \"Add secret\"\n\nAdd test token:\n   - Click \"New repository secret\"\n   - Name: \n   - Secret: Paste TestPyPI token ()\n   - Click \"Add secret\"\n\nVerify Secrets\n\nSecrets should now appear in repository settings:\n\nSecurity: Secrets are encrypted. Even admins can't view values after creation.\n\nStep 4: First Manual Publish (Recommended)\n\nWhy Manual First Publish?\n\nEstablishes package on PyPI - Package name reserved\nAllows project-scoped token - More secure than account-scoped\nVerifies metadata - Catch issues before automation\nTests process - Ensures build and upload work\n\nManual Publish Steps\n\nBuild Package\n\nInstall Twine\n\nTwine is the official PyPI upload tool.\n\nUpload to TestPyPI First\n\nAlways test with TestPyPI before production:\n\nVerify TestPyPI Installation\n\nTest installation from TestPyPI:\n\nIf this fails, fix issues before proceeding to production PyPI.\n\nUpload to Production PyPI\n\nOnce TestPyPI installation succeeds:\n\nVerify Production Installation\n\nTest installation from production PyPI:\n\nSuccess! Package is now live on PyPI.\n\nStep 5: Update Token Scope (Security Best Practice)\n\nAfter first publish, restrict token to project-only access.\n\nWhy Project-Scoped Tokens?\n\nLeast privilege principle - Token can only publish to smartem-workspace\nReduced blast radius - If token leaks, attacker can't publish to other projects\nRecommended by PyPI - Best practice for production tokens\n\nUpdate Production Token\n\nDelete account-scoped token:\n   - Go",
      "excerpt": "PyPI Token Setup for CI/CD This guide explains how to set up PyPI and TestPyPI accounts and tokens for automated package publishing via GitHub Actions. Table of Contents Overview Prerequisites Step 1:..."
    },
    {
      "id": "doc-50",
      "title": "Run in a container",
      "href": "/docs/operations/run-container",
      "section": "Operations",
      "content": "Run in a container\n\nPre-built containers with smartem-decisions and its dependencies already\ninstalled are available on Github Container Registry.\n\nStarting the container\n\nTo pull the container from github container registry and run:\n\nTo get a released version, use a numbered release instead of .\n\nRunning services\n\nThe container can run in different modes controlled by the  environment variable:\n\nHTTP API Service\n\nMessage Queue Worker\n\nEnvironment Variables\n\n Default \n\nComplete Development Stack\n\nFor a complete development environment with database and message queue, see the Kubernetes deployment guide which provides a docker-compose-like experience.\n\nAdvanced Configuration\n\nCustom User/Group for Production Deployments\n\nThe pre-built containers run as root by default, which is suitable for development and CI/CD. For production deployments at Diamond Light Source that require access to the  filesystem, you'll need to build custom images with specific UID/GID.\n\nSee Container User Configuration for:\nBuilding containers with custom user/group settings\nMounting the  filesystem for image serving\nSecurity considerations and best practices\nTroubleshooting permission issues",
      "excerpt": "Run in a container Pre-built containers with smartem-decisions and its dependencies already installed are available on Github Container Registry. Starting the container To pull the container from gith..."
    },
    {
      "id": "doc-51",
      "title": "Setup SmartEM Workspace",
      "href": "/docs/operations/setup-smartem-workspace",
      "section": "Operations",
      "content": "Setup SmartEM Workspace\n\nThis guide explains how to use  to set up a complete SmartEM development environment.\n\nOverview\n\n is a command-line tool that automates the setup of a multi-repository workspace for SmartEM development. It handles repository cloning, Claude Code configuration, Serena MCP setup, and workspace structure creation.\n\nWhat Gets Set Up\n\nWhen you run , the tool creates:\n\nRepository clones - Organised by GitHub organisation (DiamondLightSource, FragmentScreen, GitlabAriaPHP)\nClaude Code configuration - Skills, agents, settings, and permissions for AI-assisted development\nSerena MCP server - Semantic code navigation and symbol search\nWorkspace structure - CLAUDE.md, tmp/, testdata/ directories\nDevelopment tools - Pre-configured for the SmartEM ecosystem\n\nWhen to Use This Tool\n\nUse  when you need to:\n\nSet up a new SmartEM development environment\nClone multiple related repositories at once\nConfigure Claude Code for SmartEM development\nStandardise workspace layout across team members\nQuickly bootstrap a dev environment on a new machine\n\nPrerequisites\n\nBefore using , ensure you have:\n\nPython 3.11 or later - Check with  or \nGit - For repository cloning\nuv or uvx - Modern Python package installer (install guide)\nInternet connection - For cloning repositories and fetching configuration\nGitHub access - Public repos work without authentication; private repos need credentials\n\nInstallation Methods\n\nMethod 1: Run Directly with uvx (Recommended)\n\nNo installation needed.  downloads and runs the tool in an isolated environment:\n\nThis is the recommended method because:\nNo permanent installation clutters your system\nAlways uses the latest version from PyPI\nIsolated environment prevents dependency conflicts\nWorks immediately without setup\n\nMethod 2: Install Globally with uv\n\nInstall once, use repeatedly:\n\nUse this method if:\nYou'll run the tool frequently\nYou want tab completion for the command\nYou prefer traditional installed tools\n\nMethod 3: Install with pipx\n\nAlternative to uv for those who prefer pipx:\n\nTroubleshooting Installation\n\n\"uvx: command not found\"\n\nInstall uv first:\n\n\"No module named smartem_workspace\"\n\nEnsure you're using the correct command:\nWith uvx:  (not )\nWith installed tool: \n\n\"Cannot find package smartem-workspace on PyPI\"\n\nThe package might not be published yet. Contact the SmartEM team or use a development build.\n\nQuick Start\n\nThe simplest way to get started:\n\nThis launches an interactive wizard that:\nAsks which preset you want (or custom selection)\nConfirms the target directory\nShows what will be cloned\nClones repositories with progress indicators\nSets up Claude Code configuration\nSets up Serena MCP server\nCreates workspace structure\n\nNon-Interactive Quick Start\n\nFor automated setups or scripts:\n\nPresets Explained\n\nPresets are predefined collections of repositories optimised for different use cases.\n\nminimal\n\nSize: ~13 MB  \nRepositories: 1  \nUse Case: Workspace setup and documentation only\n\nContains only  repository. Use this if you:\nWant to explore the documentation\nNeed workspace configuration without code repositories\nAre setting up a minimal environment for testing\n\nIncluded:\nsmartem-core\n\nSize: ~31 MB  \nRepositories: 3  \nUse Case: Core SmartEM development\n\nContains the essential repositories for SmartEM development. Use this if you:\nWork on SmartEM backend or frontend\nNeed the core system for development and testing\nWant a lightweight but functional development environment\n\nIncluded:\n- Developer tooling and documentation\n- Backend (FastAPI, PostgreSQL, RabbitMQ)\n- Web UI (React, TanStack Router)\n\naria-reference\n\nSize: ~100 MB  \nRepositories: 20+  \nUse Case: ARIA ecosystem exploration and integration work\n\nContains ARIA backend and FandanGO plugin repositories. Use this if you:\nWork on FandanGO-cryoem-dls (ARIA deposition)\nNeed to understand ARIA integration\nAre developing facility plugins\n\nIncluded:\nAll  repositories\nAll  repositories (PHP libraries)\nPeer facility plugins (CNB, CERM, GUF)\n\nfull\n\nSize: ~150 MB  \nRepositories: 30+  \nUse Case: Complete ecosystem development\n\nContains all repositories across all organisations. Use this if you:\nWork across multiple SmartEM components\nNeed complete context for architectural work\nAre doing cross-repository refactoring\nWant all reference code available\n\nIncluded:\nAll DiamondLightSource repositories\nAll FragmentScreen repositories\nAll GitlabAriaPHP repositories\n\nConfiguration Options\n\n--path: Target Directory\n\nSpecify where to create the workspace:\n\nDefault: Current directory ()\n\nBehaviour:\nCreates directory if it doesn't exist\nFails if directory exists and is not empty (safety measure)\nCreates , , ,  subdirectories\n\n--preset: Skip Repository Selection\n\nUse a predefined preset instead of interactive selection:\n\nDefault: None (interactive selection)\n\nBehaviour:\nSkips repository selection prompts\nShows summary of what will be cloned\nStill prompts for confirmation unless  is used\n\n--no-interactive: Fully Automated Mode\n\nSkip all prompts for ",
      "excerpt": "Setup SmartEM Workspace This guide explains how to use to set up a complete SmartEM development environment. Overview is a command-line tool that automates the setup of a multi-repository workspace fo..."
    }
  ]
}
